[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a lecturer in Applied Statistics at the University of Aberdeen where I teach and conduct research in medicine and biology. My academic interests include spatio-temporal modelling, Bayesian heirarchical modelling, hidden Markov models, amongst others.\n\n\n\nBI3010 – 3rd year statistics course covering, R, linear and generalised linear models, ggplot2, basics of causal inference.\nPGR-GLM - A PhD GLM course\n\n\n\n\nHere are a few interactive apps I’ve developed:\n\nDistributions App\nAberdeen House Prices\nAberdeen Rental Prices\n\n\n\n\nYou can reach me at deon[dot]roos4[at]abdn.ac.uk or visit my github page."
  },
  {
    "objectID": "about.html#dr.-deon-roos",
    "href": "about.html#dr.-deon-roos",
    "title": "About",
    "section": "",
    "text": "I am a lecturer in Applied Statistics at the University of Aberdeen where I teach and conduct research in medicine and biology. My academic interests include spatio-temporal modelling, Bayesian heirarchical modelling, hidden Markov models, amongst others.\n\n\n\nBI3010 – 3rd year statistics course covering, R, linear and generalised linear models, ggplot2, basics of causal inference.\nPGR-GLM - A PhD GLM course\n\n\n\n\nHere are a few interactive apps I’ve developed:\n\nDistributions App\nAberdeen House Prices\nAberdeen Rental Prices\n\n\n\n\nYou can reach me at deon[dot]roos4[at]abdn.ac.uk or visit my github page."
  },
  {
    "objectID": "bi3010.html",
    "href": "bi3010.html",
    "title": "BI3010: Statistical Analysis of Biological Data",
    "section": "",
    "text": "This 3rd year undergraduate course introduces students to key statistical theory and data analysis techniques in biological sciences. It makes strong use of R and ggplot2 but aims to give a strong fundamental and theoretical understanding of statistical modelling and associated topics.\n\n\n\n\n\nWhy do we need statistics?\n\nIntroduction to linear models (LM) and hidden assumptions\n\nThe theory of effective data visualisation\n\nR and ggplot2\n\nLMs with a continuous predictor\n\nInterpreting LMs with a continuous predictor\n\nAssessing assumptions of LMs with a continuous predictor\n\nLMs with a categorical predictor\n\nInterpreting LMs with a categorical predictor\n\nAssessing assumptions of LMs with a categorical predictor\n\nLMs with multiple predictors\n\nConfounding and causality\n\nNull Hypothesis Significance Testing and P-values\n\nAn Information Criterion (AIC)\n\nAn introduction to GLMs\n\nPoisson GLMs\n\n\n\n\n\n\n\n\n\nWhat students learn"
  },
  {
    "objectID": "bi3010.html#overview",
    "href": "bi3010.html#overview",
    "title": "BI3010: Statistical Analysis of Biological Data",
    "section": "",
    "text": "This 3rd year undergraduate course introduces students to key statistical theory and data analysis techniques in biological sciences. It makes strong use of R and ggplot2 but aims to give a strong fundamental and theoretical understanding of statistical modelling and associated topics.\n\n\n\n\n\nWhy do we need statistics?\n\nIntroduction to linear models (LM) and hidden assumptions\n\nThe theory of effective data visualisation\n\nR and ggplot2\n\nLMs with a continuous predictor\n\nInterpreting LMs with a continuous predictor\n\nAssessing assumptions of LMs with a continuous predictor\n\nLMs with a categorical predictor\n\nInterpreting LMs with a categorical predictor\n\nAssessing assumptions of LMs with a categorical predictor\n\nLMs with multiple predictors\n\nConfounding and causality\n\nNull Hypothesis Significance Testing and P-values\n\nAn Information Criterion (AIC)\n\nAn introduction to GLMs\n\nPoisson GLMs\n\n\n\n\n\n\n\n\n\nWhat students learn"
  },
  {
    "objectID": "bi3010.html#tools-software",
    "href": "bi3010.html#tools-software",
    "title": "BI3010: Statistical Analysis of Biological Data",
    "section": "Tools & Software",
    "text": "Tools & Software\n\nR\nRStudio\nggplot2\n\n\n\\(\\leftarrow\\) Back to all courses"
  },
  {
    "objectID": "blog/2024-04-27-ucas-analysis.html",
    "href": "blog/2024-04-27-ucas-analysis.html",
    "title": "Undergraduate UCAS Applications and Acceptances Analysis",
    "section": "",
    "text": "Universities across the UK are in a pretty precarious position right now. After a host of issues, post 2020 has seen the number of students fall. The well seems to be drying up.\nAnd in some cases dramatically. Coventry University made a £54 million loss in 2023-24. Dundee was talking about having to fire ca. 600 employees because apparently the previous senior management team weren’t too good at maths.\nNow, possibly in combination with inflation and the “cost-of-living-crisis”, the long term viability of some universities is looking… Grim.\nThis got me thinking over the weekend as to why. What’s going on with high-school students that makes it less attractive to go to uni? I don’t know and I can’t answer this. I’d need to do a survey of school kids and universities would never pay to get evidence to make a decision. There’s no money remember? Unless it’s for an external consultancy team to tell you if you were fire red or earth green.\nWhat I can do is look at some larger scale patterns with some crude data. It’s something, I guess. I think most people would point to faltering university recruitment and see a clear line between Brexit (and latterly, the Tories not being the most welcoming to foreigners) and the loss of the pool of European applicants.\nBut if that’s the case, then we shouldn’t see any similar kinds of trends in UK students applying to UK universities, right? Their numbers should be relatively stable. If you’re in Englandshire, why would you care that you can’t go to Europe if you were planning to go to Englandshire University regardless?\nSo that’s the idea and my question. Is there something a bit more gloabl going on than just little Britain? If UK students are relatively stable, while EU applicants fall, then it’s probably mostly because of Brexit (and associated policies) that uni recruitment is falling. And strategies to target non-EU international students may well work. But if not? Well, that’s the more “global thing”, right?\n\n\nI’ll be skipping the data processing and going straight into the analysis. As such, I will only be using a few packages:\n\n\nCode\nlibrary(ggplot2)\nlibrary(mgcv)\n\n# DT is just for including data in the blog\nlibrary(DT)\n\n\nTo look into this I pulled some data off of the UCAS website. The UCAS site is a bit of a nightmare to navigate and an even bigger cluster fuck to figure out which of the seemingly gajillions of .csvs they’ve thrown into a zip file contain relevant information I want.\nRest assured, I have been through the valley of cluster fuckery and have emerged with a dataset. I won’t subject you to the Olympic levels of gymnastics I went through to get this but if you want a visual summary, watch The Book of Eli.\nHere’s the data. All clean ’n nice like.\n\n\nCode\nucas &lt;- read.csv(\"data/UCAS.csv\", stringsAsFactors = TRUE, header = TRUE)\ndatatable(ucas, options = list(pageLength = 10, scrollX = TRUE))"
  },
  {
    "objectID": "blog/2024-04-27-ucas-analysis.html#deets",
    "href": "blog/2024-04-27-ucas-analysis.html#deets",
    "title": "Undergraduate UCAS Applications and Acceptances Analysis",
    "section": "",
    "text": "I’ll be skipping the data processing and going straight into the analysis. As such, I will only be using a few packages:\n\n\nCode\nlibrary(ggplot2)\nlibrary(mgcv)\n\n# DT is just for including data in the blog\nlibrary(DT)\n\n\nTo look into this I pulled some data off of the UCAS website. The UCAS site is a bit of a nightmare to navigate and an even bigger cluster fuck to figure out which of the seemingly gajillions of .csvs they’ve thrown into a zip file contain relevant information I want.\nRest assured, I have been through the valley of cluster fuckery and have emerged with a dataset. I won’t subject you to the Olympic levels of gymnastics I went through to get this but if you want a visual summary, watch The Book of Eli.\nHere’s the data. All clean ’n nice like.\n\n\nCode\nucas &lt;- read.csv(\"data/UCAS.csv\", stringsAsFactors = TRUE, header = TRUE)\ndatatable(ucas, options = list(pageLength = 10, scrollX = TRUE))"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "My graveyard of ideas.\n\n\n\n\n\n\n\n\n\n\n\n\n\nUndergraduate UCAS Applications and Acceptances Analysis\n\n\n\n\n\n\n`Apr 27, 2024`{=html}\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "courses.html#undergraduate-teaching",
    "href": "courses.html#undergraduate-teaching",
    "title": "Deon Roos",
    "section": "Undergraduate Teaching",
    "text": "Undergraduate Teaching\n\n  \n    BI3010: Statistical Analysis of Biological Data\n    This course covers key statistical concepts and theory for biological data using R.\n    Learn more"
  },
  {
    "objectID": "courses.html#postgraudate-teaching",
    "href": "courses.html#postgraudate-teaching",
    "title": "Deon Roos",
    "section": "Postgraudate Teaching",
    "text": "Postgraudate Teaching\n\n  \n    PGR Course: Generalised Linear Models\n    An advanced course on GLMs designed for postgrads across disciplines using R.\n    Learn more"
  },
  {
    "objectID": "cv-awards.html",
    "href": "cv-awards.html",
    "title": "Awards & Funding",
    "section": "",
    "text": "Best Student Presentation, University of Aberdeen\n2019\nTravelling waves as a form of spatio-temporal asynchrony\n\n\nBest Presentation, Iberian Vole Workshop, Porto\n2019\nSpatio-temporal asynchrony in a cyclic vole species\n\n\nSir Maitland Mackie Scholarship, University of Aberdeen\n2017\nFunding for agricultural research\n\n\nOutstanding BSc Zoology Student, University of Aberdeen\n2016\n\n\nPeople’s Trust Internship, PTES\n2016\nFunded research after BSc thesis\n\n\nCarnegie Summer Scholarship\n2015\nSupported BSc data collection\n\n\\(\\leftarrow\\) Back to CV Overview"
  },
  {
    "objectID": "cv-education.html",
    "href": "cv-education.html",
    "title": "Education",
    "section": "",
    "text": "PhD Ecology, University of Aberdeen\n2016 – 2021\n- Rodent pest population dynamics\n- Publications: Roos et al. (2019), Roos et al. (2022)\n\n\nBSc (Hons) Zoology, University of Aberdeen\n2012 – 2016\n- Graduated First Class\n\n\\(\\leftarrow\\) Back to CV Overview"
  },
  {
    "objectID": "cv-employment.html",
    "href": "cv-employment.html",
    "title": "Employment",
    "section": "",
    "text": "Lecturer in Applied Statistics, University of Aberdeen\nJanuary 2024 – Present\nAberdeen, Scotland\n- Leading and collaborating interdisciplinary research in applied statistics with real-world impact.\n- Teaching statistical theory and methods across undergraduate to PhD levels.\n- Assisting in research projects through the application of collaborator-specified statistical approaches.\n\n\nSenior Statistical Ecologist, Animal & Plant Health Agency\nMay 2022 – January 2024\nSand Hutton, England\n- Led statistics and mentoring, developing bespoke methods for policy-driven projects across multidisciplinary teams.\n\n\nTeaching Fellow, University of Aberdeen\nDec 2021 – May 2022\n- Coordinated and delivered a highly rated, fully online undergraduate statistics course with strong student engagement.\n\n\nResearch Fellow, University of Aberdeen\nJun 2021 – Feb 2022\n- Conducted international ecological research, including study design and simulation modeling for species management.\n\n\nTeaching Fellow, University of Aberdeen\nDec 2020 – May 2021\n- Course coordinator for undergraduate statistical course.\n\n\nOperating Department Support Worker, NHS Grampian\n2008 – 2012\nElgin, Scotland\n- Assisted in surgical procedures (non-sterile support).\n\n\\(\\leftarrow\\) Back to CV Overview"
  },
  {
    "objectID": "cv-phd.html",
    "href": "cv-phd.html",
    "title": "PhD Students",
    "section": "",
    "text": "Seungyeon Lee, University of Aberdeen\n2024 – Present\n- Integrated population models of tawny owls.\n\n\\(\\leftarrow\\) Back to CV Overview"
  },
  {
    "objectID": "cv-skills.html",
    "href": "cv-skills.html",
    "title": "Skills",
    "section": "",
    "text": "Programming: R, RShiny, basic Python\n\nStatistical Methods: Spatio-temporal modelling, Bayesian inference, Hidden Markov models, non-linear modelling, hierarchical modelling, simulation-based inference\n\nData Visualisation: ggplot2, gganimate, plotly, leaflet\n\nData Wrangling: comfortable with tidyverse and base R approaches; experience using SQL (SQLite via R)\n\nTeaching & Communication: In-person and online delivery, course coordination (ca. 150 students), course administration, interactive and accessible teaching of complex methods, supervision of postgraduate and PhD students\n\nReproducible Research: RMarkdown, Quarto, version control (Git), simulation workflows, reproducible project structure\n\nVersion Control: Git (via GitHub and GitHub Pages)\n\nMarkup & Documentation: RMarkdown, LaTeX, Quarto\n\nAPIs & Data Integration: Working with RESTful APIs, JSON parsing, ORCID integration\n\nLanguages: English (Fluent), Afrikaans (Fluent), Spanish (Basic)\n\n\\(\\leftarrow\\) Back to CV Overview"
  },
  {
    "objectID": "cv.html#sections",
    "href": "cv.html#sections",
    "title": "\nDeon Roos\n",
    "section": "Sections",
    "text": "Sections\n\n  \n    Employment\n    Work history across academia, statistical roles, and non-academic jobs.\n    View section\n  \n\n  \n    Education\n    Formal degrees and institutions attended.\n    View section\n  \n\n  \n    PhD Students\n    Supervision roles and ongoing projects.\n    View section\n  \n\n  \n    Awards & Funding\n    Recognition and research funding received.\n    View section\n  \n\n  \n    Skills\n    Technical, statistical, and communication expertise.\n    View section"
  },
  {
    "objectID": "explainers.html#models-concepts",
    "href": "explainers.html#models-concepts",
    "title": "Explainers",
    "section": "Models & Concepts",
    "text": "Models & Concepts\n\n\n\nIntroduction to GLMs\n\n \n\nBayesian Occupancy Models\n\n \n\nStructural Topic Models"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nI’m Deon Roos. I do stuff.\n",
    "section": "",
    "text": "I’m Deon Roos. I do stuff.\n\n\nCheck out my stuff to see what.\n\n\nLecturer | Scientist | Statistician\n\n\n\nload(“courses”) view(“output”) browse(“cv”)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\mu_i = f(\\rho_{0,i}) + f(\\rho_{1,i}) + f(\\rho_{2,i})\n\\]\n\\[\n\\rho_{j,i} = T + \\left(\\frac{1}{\\zeta_{j,i}}\\right) \\times D_{j,i}\n\\quad \\text{for } j \\in \\{0, 1, 2\\}\n\\]\n\\[\nD_{j,i} = -\\sqrt{(x_j - x_i)^2 + (y_j - y_i)^2}\n\\]\n\\[\n\\zeta_{j,i} = \\beta_0 + \\beta_1 \\times z_i\n\\]"
  },
  {
    "objectID": "pgr-glm.html",
    "href": "pgr-glm.html",
    "title": "PGR Course: Generalised Linear Models",
    "section": "",
    "text": "This advanced course focuses on the theory and application of Generalised Linear Models, tailored for PhD students and researchers. The course is designed to be non-field specific meaning it can be taken by geologists to marketers and any in between.\n\n\n\nIntroduction to statistical modelling and GLMs\n\nPoisson GLMs\n\nBinomial GLMs\n\nBernoulli GLMs"
  },
  {
    "objectID": "pgr-glm.html#overview",
    "href": "pgr-glm.html#overview",
    "title": "PGR Course: Generalised Linear Models",
    "section": "",
    "text": "This advanced course focuses on the theory and application of Generalised Linear Models, tailored for PhD students and researchers. The course is designed to be non-field specific meaning it can be taken by geologists to marketers and any in between.\n\n\n\nIntroduction to statistical modelling and GLMs\n\nPoisson GLMs\n\nBinomial GLMs\n\nBernoulli GLMs"
  },
  {
    "objectID": "pgr-glm.html#learning-objectives",
    "href": "pgr-glm.html#learning-objectives",
    "title": "PGR Course: Generalised Linear Models",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand the core structure of GLMs\nFit models using R\nTranslate statistical equations into model syntax\nInterpret coefficients and results with confidence\nRecognise when assumptions are violated"
  },
  {
    "objectID": "pgr-glm.html#course-website",
    "href": "pgr-glm.html#course-website",
    "title": "PGR Course: Generalised Linear Models",
    "section": "Course Website",
    "text": "Course Website\n🔗 Visit the course website\n\n\\(\\leftarrow\\) Back to all courses"
  },
  {
    "objectID": "publications.html#web-applications",
    "href": "publications.html#web-applications",
    "title": "Deon Roos",
    "section": "Web Applications",
    "text": "Web Applications\n\n\n\nStatistical Distributions\n\n \n\nPredicting House Prices\n\n \n\nPredicting Rent Prices"
  },
  {
    "objectID": "publications.html#peer-reviewed-papers",
    "href": "publications.html#peer-reviewed-papers",
    "title": "Deon Roos",
    "section": "Peer-reviewed papers",
    "text": "Peer-reviewed papers\nCristiano Tiberi, Sarah Beatham, Julia Coats, Izzy Rochester, Deon Roos, Riccardo Primi, Andrea Vitali, Giovanna Massei (2025). An assessment of whether age, sex, and reproductive status affect bait uptake by grey squirrels, Journal of Wildlife Management.Tiago Crispim-Mendes, Deon Roos, Clara Mendes Ferreira, Joana Paupério, João Paulo Silva, Sérgio Godinho, Paulo Célio Alves, António Mira, Pedro Beja, Xavier Lambin, Ricardo Pita (2024). Patch spatial attributes and time to disturbance affect the emergence of source local populations within ephemeral habitats, Ecological Modelling.Ijeoma Nnenna Okoliegbe, Sara Aly Abdelfatah Sharaf, Susanth Alapati, Deon Roos, Antonio Ribeiro, Istifanus Nkene, Daniele Ghezzi, Stuart Reid, Victoria Austin, Dolapo Ayansina, Rebecca Wilson, Tanzeel Ur-Rehman, Benjamin J Parcell, Ian Mellor, Charis A. Marwick, Marco Oggioni, Karolin Hijazi (2024). Effects of universal versus targeted chlorhexidine skin decolonisation on the clinical and molecular epidemiology of Staphylococcus epidermidis bloodstream infections in intensive care, SSRN Preprints with The Lancet.J. Colomer, G. Massei, Deon Roos, C. Rosell, J. D. Rodríguez-Teijeiro (2024). What drives wild boar density and population growth in Mediterranean environments?, Science of the Total Environment.Deon Roos, Constantino Caminero‐Saldaña, David Elston, François Mougeot, María Carmen García‐Ariza, Beatriz Arroyo, Juan José Luque‐Larena, Francisco Javier Rojo Revilla, Xavier Lambin (2022). From pattern to process? Dual travelling waves, with contrasting propagation speeds, best describe a self‐organised spatio‐temporal pattern in population growth of a cyclic rodent, Ecology Letters.James Slingsby, Beth E. Scott, Louise Kregting, Jason McIlvenny, Jared Wilson, Ana Couto, Deon Roos, Marion Yanez, Benjamin Williamson (2021). Surface Characterisation of Kolk-Boils within Tidal Stream Environments Using UAV Imagery, Journal of Marine Science and Engineering.Roos, D., Caminero Saldaña, C., Arroyo, B., Mougeot, F., Luque-Larena, J.J., Lambin, X. (2019). Unintentional effects of environmentally-friendly farming practices: Arising conflicts between zero-tillage and a crop pest, the common vole (Microtus arvalis), Agriculture, Ecosystems and Environment."
  },
  {
    "objectID": "publications.html#textbooks",
    "href": "publications.html#textbooks",
    "title": "Deon Roos",
    "section": "Textbooks",
    "text": "Textbooks\nA Douglas, D Roos, F Mancini, A Couto, D Lusseau. An introduction to R."
  },
  {
    "objectID": "stm-explainer.html",
    "href": "stm-explainer.html",
    "title": "Structural Topic Models",
    "section": "",
    "text": "Hey Grace,\nI’ve written this page for you and your honours project. I’ve aimed to keep it accessible, covering both the core theory behind the stats you’ll use (Structural Topic Models, STMs) but also how to actually implement them. That said, the theory can get complex at times, and I’m still learning it myself! (It’s also surprisingly hard to find good, clear explanations online.) It took me a while to make sense of the stats, so don’t worry if it also takes you some time. Be patient with when learning this material.\nTo be clear, this page isn’t meant to replace our meetings or turn me into a hands-off “supervisor.” It’s a resource you can return to whenever you need a refresher or some help getting unstuck.\nHopefully, this helps you get a handle on the method, but let me know if anything is confusing. It probably won’t answer every question, and I won’t be offended if you want to toss it in a bonfire. Just flag anything that’s unclear, and we’ll work through it together."
  },
  {
    "objectID": "stm-explainer.html#what-is-a-structural-topic-model",
    "href": "stm-explainer.html#what-is-a-structural-topic-model",
    "title": "Structural Topic Models",
    "section": "What is a Structural Topic Model?",
    "text": "What is a Structural Topic Model?\nA Structural Topic Model (STM) is a type of analysis that allows us to explore large sets of text documents and identify what the common themes are; called topics. Just like you did in BI3010, you can also include covariates (also called explanatory variables) to see if that makes a topic may be more or less prevalent.\n\nGrace, in your case, this might include things like how date (i.e. how has coverage of lynx changed over time), and if the illegal release of lynx (and maybe their interaction) may have changed which topics are more or less prevelant. For example, did negative topics become more common in the media after the illegal release compared to before?\n\nSTMs are a fairly complex beast, with lots of new ideas. One of these new ideas that I won’t explain in this document is Bayesian statistics. Luckily, I have written another set of documents that explain the general theory of this, which you are welcome and encouraged to read through. Although STMs, as implemented in the R package stm, use the Bayesian statistical framework, you can’t actually interact with it, so it’s not crucial to understand in this case. However, I would recommend trying to wrap your head around it, as it’s a piece of knowledge that may make you highly employable.\nWith that said, let’s go over, conceptually, what Structural Topic Models are going.\n\nWe begin by gathering a corpus. This is a collection of documents, like newspaper articles. Our objective is to learn something about the corpus.\nWe assume that within each document, there can exist multiple topics. Topics are the “themes” that the document covers; things like “Lynx are bad and we shouldn’t release them” or “Lynx are good and we should release them”.\nThese topics are latent, which means “hidden” or “unobserved” (newspapers don’t add a sticker on each article to say what the theme is afterall) and we want to use the STM to identify them and to see which are most prevalent.\nWe state how many theme we think there are. This might be 5 or it might be 200. This is a choice we make. (There are some tools that can help make this choice.)\nWithin each document we consider each word. We assume that each word is associated with one of these topics with differing probabilities. For example, if there is a topic for “Lynx are bad”, then we might expect that “livestock” has a 90% chance of belonging to this topic, while “rewilding” only has a 0.5% chance.\nEach topic will have a distribution of words associated with it, each with their own probability to belong to that topic.\n\nSo our objective then, is to identify different topics, and which words tend to categorise those topics. This is what we’re, fundamentally, trying to do in an STM."
  },
  {
    "objectID": "stm-explainer.html#what-does-an-stm-look-like",
    "href": "stm-explainer.html#what-does-an-stm-look-like",
    "title": "Structural Topic Models",
    "section": "What does an STM look like?",
    "text": "What does an STM look like?\nLet’s start with a simple way to visualise the data and output of an STM (apologies for the generative AI image):\n\nThe things to take away from this are to highlight that \\(d\\) is just the current document you are looking at. \\(\\theta_k\\) describes the relative proportion of a document that is dedicated to topic \\(k\\) (e.g. in the above figure we have three topics, called A, B and C). These topics are determined by the word (\\(w\\), given all words \\(n\\) used in the document \\(d\\)) within them, that appears in the topic (\\(k\\)) with probability \\(\\beta\\).\nThat’s the simplified version. If we dive into the details, things get a bit more complex."
  },
  {
    "objectID": "stm-explainer.html#the-equations",
    "href": "stm-explainer.html#the-equations",
    "title": "Structural Topic Models",
    "section": "The equations",
    "text": "The equations\nWe’ll go through this step-by-step because the estimation process in structural topic modeling is complex (but powerful).\n\n1. Document Topic Model\nOur first objective is to understand, within a document, what proportion are given to each topic where we can have multiple topics (up to \\(K\\) topics, e.g. 20). For example, an article may dedicate 50% to the topic “lynx are bad” and 50% to “protect livestock”. Why are these the topics that we see? Well maybe the newspaper has a particular politically leaning, and newspapers that have this stance tend to include these topics. Maybe this was a few days after the illegal lynx release.\nThis first “sub-model” attempts to resolve that. The complication is that we will have multiple topics, and we need a proportion for each. In BI3010 the distributions you worked with, for example the Normal distribution, which only had a single average. To model how much of each topic is in a document, we need to estimate a proportion for each topic. But before we can turn these into proportions, we first create a set of ‘topic scores’, one for each topic. These scores are drawn from a distribution that lets topics vary together (some may be more likely to appear together), and for that, we use a multivariate normal distribution, which is just a version of the normal distribution that handles multiple, possibly correlated, values at once. This model will produce multiple averages, which we’ll label with \\(\\theta\\) (note that because there are multiple values in \\(\\theta\\), we label it as \\(\\vec{\\theta}\\)).\nSo this first sub-model is a type of Generalised Linear Model, except this GLM does not use a Poisson, like you did in BI3010, but the Multivariate Normal.\nHere’s how we describe it “formally”:\nFor each document \\(d\\) that we have, with covariates \\(\\vec{X}_d\\), we work with a vocabulary of size \\(V\\) (that is, \\(V\\) unique words appear across the documents), given \\(K\\) topics, which we’re going to fit using a GLM that uses a multivariate normal distribution (with a \\(\\text{softmax}\\) link function) to estimate which topic is present in a document (note that the “\\(\\vec{}\\)” symbol is shorthand for vector, or a “column of data”):\n\\[\n\\vec{\\theta} \\sim \\text{MVNorm}(\\vec{\\mu}, \\boldsymbol\\Sigma)\n\\]\n\\[\n\\text{softmax}(\\vec{\\mu}) = {X_d} \\boldsymbol{\\Gamma}\n\\]\n\nA multivariate normal distribution is like several normal distributions stacked together. So, instead of having just one mean and one variance, you have a mean and variance for each topic. The covariance matrix \\(\\Sigma\\) (which is the variance) also tells you how topics tend to co-occur, for example, maybe “Lynx are bad” often appears alongside “Predator control”.\n\nwhere \\(\\vec{X}_d\\) is a 1-by-\\(p\\) vector (your covariates), \\(\\Gamma\\) is a \\(p\\)-by-(\\(K-1\\)) matrix of coefficients (this is a way to describe all the parameters, \\(p\\), in the model) and \\(\\boldsymbol{\\Sigma}\\) is a (\\(K-1\\))-by-(\\(K-1\\)) covariance matrix.\nKeep in mind, these \\(\\theta\\) values are like scores that indicate how much each topic is ‘preferred’ in a document. We turn these scores into probabilities using a transformation called the \\(\\text{softmax}\\), which ensures they sum to 1; like proportions should.\nHere’s what the \\(\\text{MVNorm}\\) distribution looks like when we have two averages with differing correlation (the \\(\\rho\\) values) between them:\n\n\nCode\nlibrary(mvtnorm)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nmake_density_df &lt;- function(rho) {\n  mu &lt;- c(0, 0)\n  sigma &lt;- matrix(c(1, rho, rho, 1), nrow = 2)\n  \n  x &lt;- seq(-3, 3, length.out = 100)\n  y &lt;- seq(-3, 3, length.out = 100)\n  grid &lt;- expand.grid(X1 = x, X2 = y)\n  \n  grid$z &lt;- mvtnorm::dmvnorm(grid, mean = mu, sigma = sigma)\n  grid$corr &lt;- rho\n  \n  return(grid)\n}\n\ndf_all &lt;- bind_rows(\n  make_density_df(-0.8),\n  make_density_df(0),\n  make_density_df(0.8)\n)\n\nggplot(df_all, aes(x = X1, y = X2, z = z)) +\n  geom_contour_filled(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~ corr, labeller = labeller(corr = function(x) paste0(\"ρ = \", x))) +\n  coord_equal() +\n  labs(\n    x = expression(Y[1]),\n    y = expression(Y[2])\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n2. Topic-Word Model\nAt the same time, we want to determine the probability for each word to be associated with any of our topics. For instance, what is the probability that “kill”, “predator”, “rewilding” and “nature” belong to the topic “lynx are good”? That’s what the Topic-Word model is tasked with solving.\nAssume you included a document-level content covariate \\(y_d\\) (e.g. Politically Left versus Politically Right newspaper), we can form a document-specific distribution of words (as a vector, or “column” of numbers), called \\(\\boldsymbol{\\beta}\\), which represents each topic (\\(k\\)) by using:\nWe model the probability of each word as a combination of the baseline frequency of the word, how much more or less it appears in each topic, and how that might change depending on document metadata like political leaning. So we combine these effects additively in the log space, and then exponentiate to turn it back into probabilities.\n\nThe baseline word distribution (\\(m\\), i.e. how common is this word across all documents),\nThe topic specific deviation \\(\\boldsymbol{\\kappa}^{(t)}_k\\) (i.e. is that word more or less common in topic \\(k\\))\nThe covariate group deviation \\(\\boldsymbol{\\kappa}^{(c)}_{y_d}\\) (i.e. is that word more or less common in Politically Left or Right newspapers),\nAnd the interaction between the two \\(\\boldsymbol{\\kappa}^{(i)}_{y_d,k}\\) if we want one\n\nwhich we can estimate by doing:\n\\[\n\\vec{\\beta}_{d,k} \\propto exp(\\vec{m} + \\vec{\\kappa}^{(t)}_k + \\vec{\\kappa}^{(c)}_{y_d} + \\vec{\\kappa}^{(i)}_{y_d,k})\n\\]\n\nRead this as saying “the probability, \\(\\beta\\), to see a given unique word in document \\(d\\), in topic \\(k\\) is proportional to (the \\(\\propto\\) symbol) how common it is in general, as well as how common it is in the given topic and/or group”\n\nThis results in:\n\\[\n\\vec{\\beta}_{d,k} = [\\beta_{d,k,1}, \\beta_{d,k,2}, ..., \\beta_{d,k,V}]\n\\]\nwhere \\(\\vec{\\beta}_{d,k}\\) is a vector that contains the probability to see a given unique word (the \\(_{1,2,...,V}\\) bit) in a topic (\\(k\\)), in a document (\\(d\\))).\n\n\n3. Estimating \\(\\vec{\\beta}_{d,k}\\)\nKeep in mind that \\(\\vec{\\beta}_{d,k}\\) should be a probability. But to figure it out we start by estimating the rate that at which we see each unique word (\\(v\\)) across the entire corpus using multiple Poisson GLMs; one for each unique word:\n\\[\nw_v \\sim Poisson(\\lambda_v)\n\\]\n\\[\nlog(\\lambda_v) = m_v + \\kappa^{(t)}_{k,v} + \\kappa^{(c)}_{y_d,v} + \\kappa^{(i)}_{y_d,k,v}\n\\]\nHere, \\(w_v\\) is the observed count of word \\(v\\). Remember from BI3010 that a Poisson GLM estimates a rate but here we need a probability. To do that, we take the estimated rate (\\(\\lambda\\)) for word \\(v\\) and divide it by the sum all of the \\(\\lambda\\)s of all the other Poisson GLMs to get a probability (e.g. if we see the word lynx 100 times but we see all words a total of 500 times, then the probability to see the word lynx is \\(\\frac{100}{500} = 0.2 = 20\\%\\). We do this by:\n\\[\n\\beta_{d,k,v} = \\frac{\\lambda_v}{\\sum\\lambda_{v'}}\n\\]\n\nA small note here. Normally you’d want to estimate this by using a multinomial GLM, which estimates the probability of an event happening - like seeing the word lynx - but when you have lots of different words. The problem occurs when you have hundreds of thousands of unique words. In that case a multinomial model can take far too long to fit. That’s why stm uses a Poisson model for each unique word which takes these rates and converts them to probabilities.\n\n\n\n4. Which words and which topics?\nNow that we’ve estimated the topic proportions \\(\\vec{\\theta}_d\\) and the topic-word distributions \\(\\vec{\\beta}_{d,k}\\), we can now estimate the latent variables that explain how each word was chosen.\nFor each word in the document (which we can write as \\(n \\in \\{1,...,N_d\\}\\), or “for each word that is in all words from the first to the last”) :\n\nEstimate the topic by fitting a multinomial GLM, based on the probabilities in the vector \\(\\vec{\\theta}_d\\):\n\\[\nz_{d,n} \\sim Multinomial(\\vec{\\theta_d})\n\\]\nThen conditional on the topic, we fit another multinomial GLM to estimate which word is most likely to appear in that topic:\n\\[\nw_{d,n} \\sim Multinomial(\\vec{\\beta}_{d,k=z_{d,n}})\n\\]\n\nAnd that’s it. Suuuuuper simple, right? For transparency, I spent about three days going over material trying to make sense of the literature, in part because quantitative social scientists use very different terminology and a lot of the material I found glossed over the details, making it frustratingly hard to understand what an STM is actually doing. (But also a hell of a lot of fun)."
  },
  {
    "objectID": "stm-explainer.html#plate-notation",
    "href": "stm-explainer.html#plate-notation",
    "title": "Structural Topic Models",
    "section": "Plate notation",
    "text": "Plate notation\nIf the above equations were too much, there’s another way to describe how the model works; more visual and less algebraic. It doesn’t given the nuts-and-bolts but it might help to give an intuition.\nTo do so, we can use plate notation. These are diagrams that describe how different parts of the model relate to each other.\n\n\nCode\nlibrary(DiagrammeR)\n\ngrViz(\"\ndigraph stm {\n  graph [layout = dot, rankdir = LR]\n\n  # Nodes\n  Σ [shape=circle, label='Σ', style=dashed]\n  μ [shape=circle, label='μ', style=dashed]\n  X [shape=circle, label='X']\n  κ [shape=circle, label='κ']\n  θ [shape=circle, label='θ', style=dashed]\n  z [shape=circle, label='z', style=dashed]\n  w [shape=circle, label='w']\n  β [shape=circle, label='β', style=dashed]\n\n  # Edges\n  Σ -&gt; θ\n  μ -&gt; θ\n  X -&gt; θ\n  θ -&gt; z\n  z -&gt; w\n  β -&gt; w\n  κ -&gt; β\n\n  # Outer plate: D\n  subgraph cluster_D {\n    label = 'D'\n    style = 'solid'\n    X; θ; β; κ;\n\n    # Nested plate: N\n    subgraph cluster_N {\n      label = 'N'\n      style = 'solid'\n      z; w;\n    }\n  }\n}\n\")\n\n\n\n\n\n\nWhere:\n\nNodes: Circles represent variables. Dashed circles mean they are latent (a variable we have to estimate), while solid circles means they are observed data.\nPlates: Rectangle indicate repetition:\n\n\\(D\\): Each node is relevent for each document\n\\(N\\): Each node is relevant for each word (and because \\(N\\) is within \\(D\\), also for each document)\n\n\nAnd where the variables in the plate notation are:\n\n\\(X\\) - Document level covariates (e.g. date of publication, political leaning)\n\\(\\mu\\) - Mean score for each topic\n\\(\\Sigma\\) - The covariance matrix between topics (models topic co-occurence)\n\\(\\theta\\) - The estimated topic proportion (which sums to 1)\n\\(z\\) - Estimated topic assignment for word \\(n\\) in document \\(d\\)\n\\(w\\) - The actual observed word (e.g. lynx)\n\\(\\beta\\) - The estimated word distribution for topic \\(k\\)\n\\(\\kappa\\) - Document level content covariate (e.g. political group)\n\n\nWhat’s in the box?\nNow that we’ve seen the equations and model structure, let’s connect them to the key data structures that the model learns. These are the basis for most interpretations, visualizations, and inferences.\nSpecifically, STM produces two major matrices that summarise the relationship between documents, topics, and words:\n\nThe topic-word matrix \\(\\beta\\) tells us, for each topic, how likely each word is to appear; this is how we’re able to interpret what each topic is “about.”\nThe document-topic matrix \\(\\theta\\) tells us, for each document, how much it draws on each topic; this is how we understand which topics are emphasized in which texts.\n\nThese matrices are useful because they translate the complex statistical machinery into something that might be more understandable: which documents talk about which topics, and what those topics consist of.\nFor \\(\\beta\\) it’s actually a topic-word matrix \\(\\beta\\): of dimension \\(K times V\\). For example, if we have 10 topics (\\(K = 10\\)) and 100 unique words (\\(V = 100\\)), then we would have a matrix with 10 rows and 100 columns.\nWithin the matrix each row (\\(\\beta_k\\)) is the distribution of probabilities to see each unique word in turn.\nFor example, \\(\\beta\\) might look like:\n\\[\n\\begin{array}{c|cccc}\n& \\text{predator} & \\text{policy} & \\text{illegal} & \\cdots & \\text{Word V} \\\\\n\\hline\n\\text{Topic } 1 & 0.35 & 0.10 & 0.06 & \\cdots & \\beta_{1,V} \\\\\n\\text{Topic } 2 & 0.05 & 0.01 & 0.32 & \\cdots & \\beta_{2,V} \\\\\n\\text{Topic } 3 & 0.10 & 0.02 & 0.08 & \\cdots & \\beta_{3,V} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\text{Topic } K & \\beta_{K,1} & \\beta_{K,2} & \\beta_{K,3} & \\cdots & \\beta_{K,V}\n\\end{array}\n\\]\nFor \\(\\theta\\), it’s a document-topic matrix of dimension \\(D \\times K\\). For example, if we have 5 documents (\\(D = 5\\)) and 10 topics (\\(K = 10\\)), then \\(\\theta\\) would be a matrix with 5 rows and 10 columns.\nEach row \\(\\theta_d\\) is a probability distribution over topics for that document — i.e., which topics are present, and to what degree. In the Document-Topic Model above, we were estimating each row! And as a result of \\(\\text{softmax}\\) each row sums to 1.\nFor example, \\(\\theta\\) might look like:\n\\[\n\\begin{array}{c|cccccc}\n& \\text{Topic 1} & \\text{Topic 2} & \\text{Topic 3} & \\cdots & \\text{Topic 10} \\\\\n\\hline\n\\text{Doc 1} & 0.40 & 0.10 & 0.25 & \\cdots & \\theta_{1,K} \\\\\n\\text{Doc 2} & 0.05 & 0.60 & 0.10 & \\cdots & \\theta_{2,K} \\\\\n\\text{Doc 3} & 0.15 & 0.15 & 0.10 & \\cdots & \\theta_{3,K} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\text{Doc D} & \\theta_{D,1} & \\theta_{D,2} & \\theta_{D,3} & \\cdots & \\theta_{D,K}\n\\end{array}\n\\]"
  },
  {
    "objectID": "stm-explainer.html#load-packages-and-data",
    "href": "stm-explainer.html#load-packages-and-data",
    "title": "Structural Topic Models",
    "section": "Load Packages and Data",
    "text": "Load Packages and Data\nTo do the analysis we’re going to use stm to run the actual STMs, as well as tm and ggplot2 to help with text processing and visualisations.\nHere’s what our data looks like (note that I’ve shortened the articles so the screen isn’t filled with text).\n\n\nCode\nlibrary(stm)\nlibrary(tm)\nlibrary(ggplot2)\n\ndata &lt;- read.csv(\"data/poliblogs2008.csv\", stringsAsFactors = FALSE)\n\nlibrary(DT)\nlibrary(dplyr)\nlibrary(stringr)\n\npreview_data &lt;- data |&gt; \n  mutate(documents = str_trim(str_replace(documents, \"^\\\\s+|\\\\s+$\", \"\"))) |&gt;   # trim outer space\n  mutate(documents = word(documents, 1, 20, sep = \" \")) |&gt; \n  mutate(documents = paste0(documents, \" […]\"))\n\nDT::datatable(preview_data, \n              options = list(\n                pageLength = 5,\n                scrollX = TRUE\n          ))"
  },
  {
    "objectID": "stm-explainer.html#preprocess-text",
    "href": "stm-explainer.html#preprocess-text",
    "title": "Structural Topic Models",
    "section": "Preprocess Text",
    "text": "Preprocess Text\nThe first important stage in the analysis, before we get to the modelling, is to process the text. There’s apparently a lot of debate in the social sciences over whether or not to do some of these steps. I won’t lie. I’m no expert so I can’t give any meaningful advice about which ones are sensible and which aren’t, other than to suggest you do some of your own research and decide which you think are appropriate to use.\nImagine we have this sentence:\n\n100 lynx were released today in Edinburgh. Locals are said to have fed them Whiskers cat food and offered them some buckfast.\n\nAfter text processing, this becomes:\n\n100 lynx were released today in edinburgh\nlocals are said to have fed them whiskers cat food and offered them some buckfast\n\nHere are the types of text processing that can be done, and what they do:\n\n\n\n\n\n\n\n\n\nStep\nDescription\nBefore\nAfter\n\n\n\n\nLowercasing\nConvert all words to lowercase to avoid treating “Lynx” and “lynx” as different.\nEdinburgh\nedinburgh\n\n\nStopword removal\nRemove very common words that don’t carry meaning (e.g. “was”, “it”, “to”).\nwas, to, and\n(removed)\n\n\nStemming\nReduce words to their root form to group variants.\noffered, feeding\noffer, feed\n\n\nPunctuation removal\nStrip punctuation to avoid splitting on tokens like “cat.” and “cat”.\nbuckfast.\nbuckfast\n\n\nRemoving numbers\nRemove tokens that are just numbers (often meaningless in context).\n100\n(removed)\n\n\nFiltering by word length\nRemove words that are too short or too long (e.g. length &lt; 3 or &gt; 20).\nin, supercalifragilisticexpialidocious\n(removed)\n\n\nTokenization\nBreak text into individual words (“tokens”).\n\"fed it Whiskers\"\nfed, it, whiskers\n\n\nRemoving rare/common terms\nRemove words that are too rare or too frequent across documents.\nbuckfast or the\n(removed) (if threshold met)\n\n\n\nTo do this, we use the functions textProcessor() and prepDocuments(). These carry out the processing mentioned above and get’s the documents and data ready for analysis.\n\n\nCode\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n# I'm not entirely convinved by stms text processing, so these are my additions:\ndata$documents_clean &lt;- tolower(data$documents)\n\n# expand or remove contraction suffixes\ndata$documents_clean &lt;- gsub(\"'re\\\\b\", \" are\", data$documents_clean)\ndata$documents_clean &lt;- gsub(\"'ll\\\\b\", \" will\", data$documents_clean)\ndata$documents_clean &lt;- gsub(\"'ve\\\\b\", \" have\", data$documents_clean)\ndata$documents_clean &lt;- gsub(\"n't\\\\b\", \" not\", data$documents_clean)\ndata$documents_clean &lt;- gsub(\"'s\\\\b\", \"\", data$documents_clean)\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n\nprocessed &lt;- textProcessor(data$documents, metadata = data)\n\n\nBuilding corpus... \nConverting to Lower Case... \nRemoving punctuation... \nRemoving stopwords... \nRemoving numbers... \nStemming... \nCreating Output... \n\n\nCode\nout &lt;- prepDocuments(processed$documents, processed$vocab, processed$meta)\n\n\nRemoving 83198 of 123990 terms (83198 of 2298953 tokens) due to frequency \nYour corpus now has 13246 documents, 40792 terms and 2215755 tokens.\n\n\nCode\ndocs &lt;- out$documents\nvocab &lt;- out$vocab\nmeta &lt;- out$meta\n\n\nAfter this you can see the result that the processing has had on text within the documents:\n\n\nCode\n# Code below just for website, don't worry about including\ndocs_readable &lt;- lapply(docs, function(doc) {\n  words &lt;- vocab[doc[1, ]]\n  rep(words, doc[2, ])\n})\npreview_data &lt;- data.frame(\n  documents = sapply(docs_readable, function(words) {\n    snippet &lt;- paste(head(words, 20), collapse = \" \")\n    paste0(snippet, \" […]\")\n  })\n)\ndatatable(preview_data,\n          options = list(\n            pageLength = 5,\n            scrollX = TRUE\n          ))\n\n\n\n\n\n\n\nIn the table where I described how the text is processed, the final option was to remove words that are too rare or too frequent. To do so, we need a threshold; how rare is too rare? To help us with this, we can produce the figure below. This shows how many documents, words or tokens would be removed if we increased lower.thresh in prepDocuments().\n\nPanel 1: Documents removed by threshold. At all threshold values considered (from 1 to 200 documents that it must appear in), no documents would ever be removed as prepDocuments() only removes a document if none of its words survive the treshold.\nPanel 2: Words removed by threshold. This shows how many unique words would be removed from the vocabulary due to being too rare.\nPanel 3: Tokens removed by threshold. This shows how many individuals words would be removed.\n\nWhat we’re looking for here is where the relationships largely flatten out. When the relationships are steep this implies that there are a lot of rare words that won’t help us understand the topics, so we can remove these without much risk. But when they get flatter, then any additional pruning isn’t worth it.\n\n\nCode\nplotRemoved(processed$documents, lower.thresh = seq(1, 200, by = 5))\n\n\n\n\n\nI think a threshold of somewhere between 30-60 seems reasonable across the three figures, so I’ll rerun the prepDocuments() function, with lower.thresh = 50.\n\n\nCode\nout &lt;- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 50)\n\n\nRemoving 119243 of 123990 terms (359560 of 2298953 tokens) due to frequency \nYour corpus now has 13246 documents, 4747 terms and 1939393 tokens.\n\n\nCode\ndocs &lt;- out$documents\nvocab &lt;- out$vocab\nmeta &lt;- out$meta\n\n\nAnd with that, we’re done with the data processing."
  },
  {
    "objectID": "stm-explainer.html#choosing-k",
    "href": "stm-explainer.html#choosing-k",
    "title": "Structural Topic Models",
    "section": "Choosing K",
    "text": "Choosing K\nWhen we run STMs, we need to specify how many topics there are. If we choose too many, the topics become overly specific and often blur together. Too few, and they’re so broad that it’s hard to identify meaningful themes. We want a \\(K\\) value that’s just right.\nTo help with this, the function searchK() runs an STM for each value of \\(K\\) that we specify, but with a twist: it holds out some documents during training, then checks how well the model predicts them. This helps evaluate predictive performance, which we see in the held-out likelihood plot below.\nWhen we plot out the result of searchK()`, across the different \\(K\\) values we specified, we’re looking for a balance between:\n\nHeld-out Likelihood: how well the model generalizes to unseen data (higher is better)\nResiduals: how much structure is unexplained (lower is better)\nSemantic Coherence: how well topic words cluster together in real documents (higher is better)\nLower Bound: technical measure of model fit (can be ignored for model selection)\n\nWhat are we looking for?\nWe’re trying to balance multiple criteria:\n\nHigh held-out likelihood\nLow residuals\nHigh semantic coherence\nHigh exclusivity (evaluated separately after modelling)\n\n\nNote: While exclusivity is important, it is not reported by searchK(). It must be evaluated separately after fitting a model using stm() and can only be done for models that do not include any content covariates (variables that influence \\(\\beta\\)).\n\n\n\n\n\n\n\n\n\nIf you see…\nIt means…\nWhat to do\n\n\n\n\nCoherence ↑ but Exclusivity ↓\nTopics are interpretable but overlapping\nTry slightly lower K\n\n\nExclusivity ↑ but Coherence ↓\nTopics are distinct but fragmented or incoherent\nTry slightly lower or slightly higher K\n\n\nHeld-out Likelihood levels off\nPredictive gains are saturated\nGood point to stop increasing K\n\n\nResiduals flatten\nModel has explained most of the structure\nIncreasing K adds noise, not insight\n\n\n\n\nNote that this might take a long time to run.\n\n\n\nCode\nres_K &lt;- searchK(docs, vocab, K = c(10, 15, 20, 25), data = meta)\n\n\nBeginning Spectral Initialization \n     Calculating the gram matrix...\n     Finding anchor words...\n    ..........\n     Recovering initialization...\n    ...............................................\nInitialization complete.\n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 1 (approx. per word bound = -7.455) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 2 (approx. per word bound = -7.352, relative change = 1.390e-02) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 3 (approx. per word bound = -7.320, relative change = 4.261e-03) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 4 (approx. per word bound = -7.307, relative change = 1.754e-03) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 5 (approx. per word bound = -7.301, relative change = 8.952e-04) \nTopic 1: obama, mccain, campaign, hillari, barack \n Topic 2: bill, legisl, hous, congress, senat \n Topic 3: bush, said, presid, report, administr \n Topic 4: democrat, obama, senat, republican, will \n Topic 5: one, like, get, just, time \n Topic 6: will, govern, year, can, new \n Topic 7: mccain, tax, john, said, will \n Topic 8: vote, state, elect, voter, democrat \n Topic 9: iraq, will, war, iran, militari \n Topic 10: peopl, one, will, american, right \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 6 (approx. per word bound = -7.297, relative change = 5.289e-04) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 7 (approx. per word bound = -7.294, relative change = 3.437e-04) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 8 (approx. per word bound = -7.293, relative change = 2.434e-04) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 9 (approx. per word bound = -7.291, relative change = 1.829e-04) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 10 (approx. per word bound = -7.290, relative change = 1.440e-04) \nTopic 1: obama, mccain, campaign, hillari, barack \n Topic 2: bill, hous, legisl, congress, fund \n Topic 3: bush, said, report, presid, administr \n Topic 4: democrat, republican, senat, obama, will \n Topic 5: one, like, get, just, time \n Topic 6: will, oil, govern, year, energi \n Topic 7: mccain, tax, john, palin, said \n Topic 8: vote, state, voter, elect, poll \n Topic 9: iraq, war, will, militari, iran \n Topic 10: peopl, one, american, will, right \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 11 (approx. per word bound = -7.289, relative change = 1.179e-04) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 12 (approx. per word bound = -7.289, relative change = 9.910e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 13 (approx. per word bound = -7.288, relative change = 8.634e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 14 (approx. per word bound = -7.288, relative change = 7.801e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 15 (approx. per word bound = -7.287, relative change = 7.245e-05) \nTopic 1: obama, mccain, campaign, barack, hillari \n Topic 2: bill, hous, legisl, congress, fund \n Topic 3: bush, said, report, presid, administr \n Topic 4: democrat, republican, senat, will, polit \n Topic 5: one, like, get, time, just \n Topic 6: will, oil, year, govern, energi \n Topic 7: mccain, palin, john, tax, said \n Topic 8: vote, state, poll, voter, elect \n Topic 9: iraq, war, will, militari, iran \n Topic 10: peopl, one, american, will, right \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 16 (approx. per word bound = -7.287, relative change = 6.814e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 17 (approx. per word bound = -7.286, relative change = 6.421e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 18 (approx. per word bound = -7.286, relative change = 6.101e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 19 (approx. per word bound = -7.285, relative change = 5.867e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 20 (approx. per word bound = -7.285, relative change = 5.722e-05) \nTopic 1: obama, campaign, mccain, barack, hillari \n Topic 2: bill, hous, legisl, congress, money \n Topic 3: bush, said, report, presid, administr \n Topic 4: democrat, republican, will, senat, polit \n Topic 5: one, like, get, time, just \n Topic 6: will, oil, year, govern, new \n Topic 7: mccain, palin, john, tax, said \n Topic 8: vote, state, poll, voter, democrat \n Topic 9: iraq, war, will, militari, iran \n Topic 10: peopl, american, one, will, right \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 21 (approx. per word bound = -7.284, relative change = 5.625e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 22 (approx. per word bound = -7.284, relative change = 5.558e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 23 (approx. per word bound = -7.284, relative change = 5.505e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 24 (approx. per word bound = -7.283, relative change = 5.459e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 25 (approx. per word bound = -7.283, relative change = 5.381e-05) \nTopic 1: obama, campaign, barack, mccain, hillari \n Topic 2: hous, bill, legisl, congress, money \n Topic 3: bush, said, report, presid, administr \n Topic 4: democrat, will, republican, think, presid \n Topic 5: one, like, get, time, just \n Topic 6: will, oil, year, econom, govern \n Topic 7: mccain, palin, john, said, sen \n Topic 8: vote, state, poll, voter, democrat \n Topic 9: iraq, war, will, militari, iran \n Topic 10: peopl, american, one, will, america \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 26 (approx. per word bound = -7.282, relative change = 5.239e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 27 (approx. per word bound = -7.282, relative change = 5.009e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 28 (approx. per word bound = -7.282, relative change = 4.722e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 29 (approx. per word bound = -7.281, relative change = 4.415e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 30 (approx. per word bound = -7.281, relative change = 4.086e-05) \nTopic 1: obama, campaign, barack, hillari, clinton \n Topic 2: hous, bill, legisl, senat, congress \n Topic 3: bush, said, report, administr, presid \n Topic 4: democrat, will, think, republican, presid \n Topic 5: one, like, get, time, media \n Topic 6: will, tax, year, oil, econom \n Topic 7: mccain, john, palin, said, sen \n Topic 8: vote, state, democrat, poll, voter \n Topic 9: iraq, war, will, militari, iran \n Topic 10: peopl, american, one, will, america \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 31 (approx. per word bound = -7.281, relative change = 3.714e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 32 (approx. per word bound = -7.281, relative change = 3.300e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 33 (approx. per word bound = -7.280, relative change = 2.842e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 34 (approx. per word bound = -7.280, relative change = 2.430e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 35 (approx. per word bound = -7.280, relative change = 2.064e-05) \nTopic 1: obama, campaign, barack, hillari, clinton \n Topic 2: hous, bill, senat, legisl, congress \n Topic 3: bush, said, report, administr, presid \n Topic 4: think, will, peopl, democrat, presid \n Topic 5: one, like, get, time, media \n Topic 6: will, tax, american, year, econom \n Topic 7: mccain, john, palin, said, sen \n Topic 8: vote, democrat, state, poll, voter \n Topic 9: iraq, war, will, militari, iran \n Topic 10: peopl, american, one, will, america \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 36 (approx. per word bound = -7.280, relative change = 1.772e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 37 (approx. per word bound = -7.280, relative change = 1.543e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 38 (approx. per word bound = -7.280, relative change = 1.364e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 39 (approx. per word bound = -7.280, relative change = 1.194e-05) \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nCompleting Iteration 40 (approx. per word bound = -7.279, relative change = 1.056e-05) \nTopic 1: obama, campaign, barack, hillari, clinton \n Topic 2: hous, bill, senat, legisl, congress \n Topic 3: bush, said, report, administr, presid \n Topic 4: think, peopl, will, like, presid \n Topic 5: one, like, get, time, media \n Topic 6: will, tax, american, econom, year \n Topic 7: mccain, john, palin, said, sen \n Topic 8: vote, democrat, state, poll, voter \n Topic 9: iraq, war, will, militari, iran \n Topic 10: peopl, american, one, will, america \n....................................................................................................\nCompleted E-Step (3 seconds). \nCompleted M-Step. \nModel Converged \nBeginning Spectral Initialization \n     Calculating the gram matrix...\n     Finding anchor words...\n    ...............\n     Recovering initialization...\n    ...............................................\nInitialization complete.\n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 1 (approx. per word bound = -7.421) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 2 (approx. per word bound = -7.311, relative change = 1.471e-02) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 3 (approx. per word bound = -7.280, relative change = 4.276e-03) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 4 (approx. per word bound = -7.266, relative change = 1.897e-03) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 5 (approx. per word bound = -7.259, relative change = 1.026e-03) \nTopic 1: obama, mccain, campaign, barack, john \n Topic 2: bill, legisl, vote, congress, hous \n Topic 3: bush, presid, said, report, hous \n Topic 4: democrat, senat, republican, obama, will \n Topic 5: get, one, like, ’re, ’ll \n Topic 6: will, govern, can, peopl, american \n Topic 7: tax, mccain, economi, will, econom \n Topic 8: vote, elect, state, voter, court \n Topic 9: will, oil, attack, russia, govern \n Topic 10: one, will, peopl, like, can \n Topic 11: school, citi, said, report, polic \n Topic 12: palin, think, say, like, mccain \n Topic 13: obama, hillari, clinton, mccain, democrat \n Topic 14: iran, israel, bush, presid, nuclear \n Topic 15: iraq, iraqi, war, troop, militari \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 6 (approx. per word bound = -7.254, relative change = 6.118e-04) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 7 (approx. per word bound = -7.252, relative change = 3.915e-04) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 8 (approx. per word bound = -7.250, relative change = 2.670e-04) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 9 (approx. per word bound = -7.248, relative change = 1.936e-04) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 10 (approx. per word bound = -7.247, relative change = 1.485e-04) \nTopic 1: obama, mccain, campaign, barack, john \n Topic 2: bill, vote, legisl, congress, senat \n Topic 3: bush, presid, said, administr, report \n Topic 4: democrat, republican, senat, parti, will \n Topic 5: get, one, ’re, like, don’t \n Topic 6: will, govern, can, need, american \n Topic 7: tax, economi, econom, plan, health \n Topic 8: vote, elect, state, voter, court \n Topic 9: will, attack, govern, russia, pakistan \n Topic 10: one, will, peopl, world, time \n Topic 11: school, citi, report, offic, state \n Topic 12: think, peopl, like, say, know \n Topic 13: obama, hillari, clinton, poll, democrat \n Topic 14: iran, israel, nuclear, bush, state \n Topic 15: iraq, war, iraqi, troop, militari \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 11 (approx. per word bound = -7.246, relative change = 1.196e-04) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 12 (approx. per word bound = -7.246, relative change = 1.009e-04) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 13 (approx. per word bound = -7.245, relative change = 8.767e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 14 (approx. per word bound = -7.244, relative change = 7.691e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 15 (approx. per word bound = -7.244, relative change = 6.649e-05) \nTopic 1: obama, mccain, campaign, barack, john \n Topic 2: bill, vote, congress, legisl, senat \n Topic 3: bush, presid, said, administr, report \n Topic 4: democrat, republican, senat, parti, will \n Topic 5: get, one, like, ’re, don’t \n Topic 6: will, can, govern, american, need \n Topic 7: tax, econom, plan, economi, financi \n Topic 8: vote, elect, state, voter, campaign \n Topic 9: attack, govern, will, russia, pakistan \n Topic 10: one, will, life, peopl, world \n Topic 11: school, report, citi, group, new \n Topic 12: think, peopl, like, say, know \n Topic 13: obama, hillari, clinton, poll, democrat \n Topic 14: iran, israel, nuclear, state, presid \n Topic 15: iraq, war, iraqi, militari, troop \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 16 (approx. per word bound = -7.243, relative change = 5.662e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 17 (approx. per word bound = -7.243, relative change = 4.779e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 18 (approx. per word bound = -7.243, relative change = 4.055e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 19 (approx. per word bound = -7.243, relative change = 3.469e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 20 (approx. per word bound = -7.242, relative change = 3.029e-05) \nTopic 1: obama, mccain, campaign, john, barack \n Topic 2: bill, vote, congress, legisl, senat \n Topic 3: bush, presid, said, administr, report \n Topic 4: democrat, republican, senat, parti, will \n Topic 5: get, one, like, ’re, don’t \n Topic 6: will, can, american, need, make \n Topic 7: tax, econom, plan, economi, financi \n Topic 8: vote, elect, voter, state, campaign \n Topic 9: attack, govern, will, russia, terrorist \n Topic 10: one, will, life, world, time \n Topic 11: school, report, group, citi, new \n Topic 12: think, peopl, like, dont, know \n Topic 13: obama, hillari, clinton, poll, democrat \n Topic 14: iran, israel, nuclear, state, terrorist \n Topic 15: iraq, war, iraqi, militari, troop \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 21 (approx. per word bound = -7.242, relative change = 2.703e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 22 (approx. per word bound = -7.242, relative change = 2.446e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 23 (approx. per word bound = -7.242, relative change = 2.274e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 24 (approx. per word bound = -7.242, relative change = 2.077e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 25 (approx. per word bound = -7.241, relative change = 1.961e-05) \nTopic 1: obama, mccain, campaign, john, barack \n Topic 2: bill, vote, congress, legisl, senat \n Topic 3: bush, presid, said, administr, hous \n Topic 4: democrat, republican, senat, parti, obama \n Topic 5: get, one, like, ’re, don’t \n Topic 6: will, american, can, need, chang \n Topic 7: tax, econom, plan, money, million \n Topic 8: vote, elect, voter, state, campaign \n Topic 9: attack, govern, will, terrorist, russia \n Topic 10: one, will, life, world, day \n Topic 11: report, school, new, group, citi \n Topic 12: think, peopl, like, dont, say \n Topic 13: obama, hillari, clinton, poll, democrat \n Topic 14: iran, israel, nuclear, state, foreign \n Topic 15: iraq, war, iraqi, militari, troop \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 26 (approx. per word bound = -7.241, relative change = 1.764e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 27 (approx. per word bound = -7.241, relative change = 1.613e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 28 (approx. per word bound = -7.241, relative change = 1.512e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 29 (approx. per word bound = -7.241, relative change = 1.455e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 30 (approx. per word bound = -7.241, relative change = 1.373e-05) \nTopic 1: obama, mccain, campaign, john, barack \n Topic 2: bill, vote, congress, senat, legisl \n Topic 3: bush, presid, said, administr, hous \n Topic 4: democrat, republican, senat, obama, parti \n Topic 5: get, one, like, ’re, don’t \n Topic 6: will, american, can, need, chang \n Topic 7: tax, econom, money, plan, million \n Topic 8: vote, elect, voter, state, campaign \n Topic 9: govern, attack, will, terrorist, forc \n Topic 10: one, will, life, world, day \n Topic 11: report, new, school, group, time \n Topic 12: think, peopl, like, dont, say \n Topic 13: obama, hillari, clinton, democrat, poll \n Topic 14: iran, israel, nuclear, state, foreign \n Topic 15: iraq, war, iraqi, militari, troop \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 31 (approx. per word bound = -7.241, relative change = 1.290e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 32 (approx. per word bound = -7.241, relative change = 1.194e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 33 (approx. per word bound = -7.241, relative change = 1.130e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 34 (approx. per word bound = -7.241, relative change = 1.094e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 35 (approx. per word bound = -7.241, relative change = 1.049e-05) \nTopic 1: obama, mccain, campaign, john, barack \n Topic 2: bill, vote, congress, senat, legisl \n Topic 3: bush, presid, said, administr, hous \n Topic 4: democrat, obama, republican, senat, parti \n Topic 5: get, one, like, ’re, don’t \n Topic 6: will, american, can, need, oil \n Topic 7: tax, money, econom, plan, million \n Topic 8: vote, elect, voter, state, campaign \n Topic 9: govern, attack, will, terrorist, forc \n Topic 10: one, will, life, day, world \n Topic 11: report, new, school, group, time \n Topic 12: think, peopl, like, dont, polit \n Topic 13: obama, hillari, clinton, democrat, poll \n Topic 14: iran, israel, nuclear, state, foreign \n Topic 15: iraq, war, iraqi, militari, troop \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 36 (approx. per word bound = -7.240, relative change = 1.036e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 37 (approx. per word bound = -7.240, relative change = 1.023e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 38 (approx. per word bound = -7.240, relative change = 1.009e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 39 (approx. per word bound = -7.240, relative change = 1.004e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nModel Converged \nBeginning Spectral Initialization \n     Calculating the gram matrix...\n     Finding anchor words...\n    ....................\n     Recovering initialization...\n    ...............................................\nInitialization complete.\n....................................................................................................\nCompleted E-Step (7 seconds). \nCompleted M-Step. \nCompleting Iteration 1 (approx. per word bound = -7.387) \n....................................................................................................\nCompleted E-Step (7 seconds). \nCompleted M-Step. \nCompleting Iteration 2 (approx. per word bound = -7.282, relative change = 1.430e-02) \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 3 (approx. per word bound = -7.251, relative change = 4.283e-03) \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 4 (approx. per word bound = -7.236, relative change = 1.977e-03) \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 5 (approx. per word bound = -7.228, relative change = 1.108e-03) \nTopic 1: obama, mccain, campaign, barack, john \n Topic 2: bill, legisl, congress, vote, hous \n Topic 3: bush, presid, hous, white, administr \n Topic 4: senat, democrat, republican, will, obama \n Topic 5: get, one, ’re, like, ’ll \n Topic 6: will, govern, market, peopl, money \n Topic 7: mccain, tax, econom, economi, health \n Topic 8: vote, elect, voter, state, ballot \n Topic 9: russia, world, georgia, nation, will \n Topic 10: one, will, peopl, like, can \n Topic 11: school, citi, said, polic, immigr \n Topic 12: mccain, think, like, say, peopl \n Topic 13: obama, mccain, poll, voter, state \n Topic 14: iran, israel, nuclear, presid, state \n Topic 15: iraq, war, iraqi, troop, militari \n Topic 16: court, law, right, tortur, will \n Topic 17: hillari, clinton, obama, democrat, will \n Topic 18: palin, report, news, media, time \n Topic 19: ayer, terrorist, obama, radic, american \n Topic 20: will, oil, energi, american, year \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 6 (approx. per word bound = -7.223, relative change = 7.021e-04) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 7 (approx. per word bound = -7.220, relative change = 4.784e-04) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 8 (approx. per word bound = -7.217, relative change = 3.386e-04) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 9 (approx. per word bound = -7.215, relative change = 2.497e-04) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 10 (approx. per word bound = -7.214, relative change = 1.897e-04) \nTopic 1: obama, mccain, campaign, barack, john \n Topic 2: bill, legisl, vote, congress, hous \n Topic 3: bush, presid, administr, said, hous \n Topic 4: democrat, senat, republican, will, parti \n Topic 5: get, one, ’re, like, don’t \n Topic 6: will, govern, money, market, financi \n Topic 7: tax, mccain, health, econom, economi \n Topic 8: vote, elect, voter, state, campaign \n Topic 9: world, nation, russia, will, georgia \n Topic 10: one, will, peopl, life, like \n Topic 11: school, citi, said, immigr, polic \n Topic 12: think, peopl, like, dont, polit \n Topic 13: mccain, poll, obama, state, voter \n Topic 14: iran, israel, nuclear, state, iranian \n Topic 15: iraq, war, iraqi, troop, militari \n Topic 16: court, law, right, tortur, constitut \n Topic 17: clinton, hillari, obama, will, democrat \n Topic 18: palin, report, news, media, time \n Topic 19: obama, ayer, terrorist, group, chicago \n Topic 20: will, oil, energi, american, global \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 11 (approx. per word bound = -7.213, relative change = 1.482e-04) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 12 (approx. per word bound = -7.212, relative change = 1.188e-04) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 13 (approx. per word bound = -7.211, relative change = 9.782e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 14 (approx. per word bound = -7.211, relative change = 8.371e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 15 (approx. per word bound = -7.210, relative change = 7.503e-05) \nTopic 1: obama, mccain, campaign, barack, john \n Topic 2: bill, vote, legisl, congress, hous \n Topic 3: bush, presid, said, administr, hous \n Topic 4: democrat, republican, senat, will, parti \n Topic 5: get, one, ’re, like, don’t \n Topic 6: will, govern, money, financi, market \n Topic 7: tax, health, econom, economi, care \n Topic 8: vote, elect, voter, campaign, state \n Topic 9: world, nation, will, russia, georgia \n Topic 10: one, will, women, life, peopl \n Topic 11: school, citi, said, offic, immigr \n Topic 12: think, peopl, like, dont, know \n Topic 13: poll, mccain, obama, state, voter \n Topic 14: iran, israel, nuclear, iranian, state \n Topic 15: iraq, war, iraqi, militari, troop \n Topic 16: law, court, right, tortur, state \n Topic 17: clinton, hillari, obama, will, democrat \n Topic 18: palin, report, news, media, time \n Topic 19: obama, group, ayer, polit, chicago \n Topic 20: will, oil, energi, global, chang \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 16 (approx. per word bound = -7.210, relative change = 6.861e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 17 (approx. per word bound = -7.209, relative change = 6.276e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 18 (approx. per word bound = -7.209, relative change = 5.933e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 19 (approx. per word bound = -7.208, relative change = 5.947e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 20 (approx. per word bound = -7.208, relative change = 6.135e-05) \nTopic 1: obama, mccain, campaign, john, barack \n Topic 2: bill, vote, legisl, congress, senat \n Topic 3: bush, presid, said, administr, hous \n Topic 4: democrat, republican, senat, parti, will \n Topic 5: get, one, like, ’re, don’t \n Topic 6: will, govern, money, financi, market \n Topic 7: tax, economi, econom, health, american \n Topic 8: vote, elect, voter, campaign, state \n Topic 9: world, nation, will, russia, countri \n Topic 10: one, will, women, life, peopl \n Topic 11: school, citi, said, offic, immigr \n Topic 12: think, peopl, like, dont, know \n Topic 13: poll, mccain, obama, state, voter \n Topic 14: iran, israel, nuclear, iranian, will \n Topic 15: iraq, war, militari, iraqi, troop \n Topic 16: law, court, right, tortur, state \n Topic 17: clinton, hillari, obama, will, democrat \n Topic 18: palin, report, news, media, time \n Topic 19: obama, group, polit, wright, ayer \n Topic 20: oil, will, energi, global, price \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 21 (approx. per word bound = -7.208, relative change = 6.258e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 22 (approx. per word bound = -7.207, relative change = 6.466e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 23 (approx. per word bound = -7.207, relative change = 6.564e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 24 (approx. per word bound = -7.206, relative change = 6.361e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 25 (approx. per word bound = -7.206, relative change = 6.096e-05) \nTopic 1: obama, mccain, campaign, john, barack \n Topic 2: bill, vote, legisl, congress, senat \n Topic 3: bush, presid, said, administr, hous \n Topic 4: democrat, republican, senat, parti, will \n Topic 5: get, like, one, ’re, don’t \n Topic 6: money, govern, financi, will, market \n Topic 7: tax, american, will, economi, econom \n Topic 8: vote, elect, voter, campaign, ballot \n Topic 9: world, nation, will, russia, countri \n Topic 10: one, will, women, life, love \n Topic 11: citi, school, said, offic, immigr \n Topic 12: think, peopl, like, dont, know \n Topic 13: poll, mccain, obama, state, voter \n Topic 14: iran, israel, nuclear, iranian, will \n Topic 15: iraq, war, militari, iraqi, troop \n Topic 16: law, court, right, state, case \n Topic 17: clinton, hillari, obama, will, democrat \n Topic 18: palin, report, media, news, time \n Topic 19: obama, group, wright, barack, polit \n Topic 20: oil, energi, will, global, price \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 26 (approx. per word bound = -7.205, relative change = 5.728e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 27 (approx. per word bound = -7.205, relative change = 5.326e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 28 (approx. per word bound = -7.205, relative change = 4.897e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 29 (approx. per word bound = -7.204, relative change = 4.423e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 30 (approx. per word bound = -7.204, relative change = 3.820e-05) \nTopic 1: mccain, obama, campaign, john, barack \n Topic 2: bill, vote, legisl, congress, senat \n Topic 3: bush, presid, said, administr, hous \n Topic 4: democrat, republican, senat, parti, will \n Topic 5: get, like, one, ’re, don’t \n Topic 6: money, financi, govern, market, will \n Topic 7: tax, will, american, economi, econom \n Topic 8: vote, elect, voter, campaign, ballot \n Topic 9: world, nation, will, countri, russia \n Topic 10: one, will, women, life, love \n Topic 11: citi, school, said, offic, polic \n Topic 12: think, peopl, like, dont, know \n Topic 13: poll, obama, mccain, state, voter \n Topic 14: iran, israel, nuclear, terrorist, iranian \n Topic 15: iraq, war, militari, iraqi, troop \n Topic 16: law, court, right, state, case \n Topic 17: clinton, hillari, obama, will, democrat \n Topic 18: palin, report, media, news, time \n Topic 19: obama, barack, group, polit, wright \n Topic 20: oil, energi, price, global, will \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 31 (approx. per word bound = -7.204, relative change = 3.318e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 32 (approx. per word bound = -7.204, relative change = 2.876e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 33 (approx. per word bound = -7.203, relative change = 2.515e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 34 (approx. per word bound = -7.203, relative change = 2.211e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 35 (approx. per word bound = -7.203, relative change = 1.970e-05) \nTopic 1: mccain, obama, campaign, john, barack \n Topic 2: bill, vote, senat, legisl, congress \n Topic 3: bush, presid, said, administr, hous \n Topic 4: democrat, republican, senat, parti, will \n Topic 5: get, like, one, ’re, don’t \n Topic 6: money, financi, govern, market, will \n Topic 7: tax, will, american, economi, econom \n Topic 8: vote, elect, voter, campaign, ballot \n Topic 9: nation, world, will, countri, govern \n Topic 10: one, will, women, life, love \n Topic 11: citi, said, school, offic, polic \n Topic 12: think, like, peopl, dont, know \n Topic 13: poll, obama, mccain, voter, state \n Topic 14: iran, israel, nuclear, terrorist, iranian \n Topic 15: iraq, war, militari, iraqi, troop \n Topic 16: law, court, right, state, case \n Topic 17: clinton, hillari, obama, will, democrat \n Topic 18: palin, media, report, news, time \n Topic 19: obama, barack, polit, wright, group \n Topic 20: oil, energi, price, global, will \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 36 (approx. per word bound = -7.203, relative change = 1.752e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 37 (approx. per word bound = -7.203, relative change = 1.558e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 38 (approx. per word bound = -7.203, relative change = 1.370e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 39 (approx. per word bound = -7.203, relative change = 1.211e-05) \n....................................................................................................\nCompleted E-Step (4 seconds). \nCompleted M-Step. \nCompleting Iteration 40 (approx. per word bound = -7.203, relative change = 1.103e-05) \nTopic 1: mccain, obama, campaign, john, barack \n Topic 2: bill, vote, senat, congress, legisl \n Topic 3: bush, presid, said, administr, hous \n Topic 4: democrat, republican, senat, parti, will \n Topic 5: get, like, one, ’re, don’t \n Topic 6: money, financi, govern, market, crisi \n Topic 7: tax, will, american, economi, health \n Topic 8: vote, elect, voter, campaign, ballot \n Topic 9: nation, world, will, countri, govern \n Topic 10: one, will, women, life, love \n Topic 11: citi, said, school, offic, polic \n Topic 12: think, like, peopl, dont, know \n Topic 13: poll, obama, voter, state, mccain \n Topic 14: iran, israel, nuclear, terrorist, attack \n Topic 15: iraq, war, militari, iraqi, troop \n Topic 16: law, court, right, state, case \n Topic 17: clinton, obama, hillari, will, democrat \n Topic 18: palin, media, news, report, time \n Topic 19: obama, barack, polit, wright, group \n Topic 20: oil, energi, price, global, will \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nCompleting Iteration 41 (approx. per word bound = -7.202, relative change = 1.024e-05) \n....................................................................................................\nCompleted E-Step (5 seconds). \nCompleted M-Step. \nModel Converged \nBeginning Spectral Initialization \n     Calculating the gram matrix...\n     Finding anchor words...\n    .........................\n     Recovering initialization...\n    ...............................................\nInitialization complete.\n....................................................................................................\nCompleted E-Step (9 seconds). \nCompleted M-Step. \nCompleting Iteration 1 (approx. per word bound = -7.351) \n....................................................................................................\nCompleted E-Step (8 seconds). \nCompleted M-Step. \nCompleting Iteration 2 (approx. per word bound = -7.250, relative change = 1.369e-02) \n....................................................................................................\nCompleted E-Step (8 seconds). \nCompleted M-Step. \nCompleting Iteration 3 (approx. per word bound = -7.220, relative change = 4.162e-03) \n....................................................................................................\nCompleted E-Step (7 seconds). \nCompleted M-Step. \nCompleting Iteration 4 (approx. per word bound = -7.205, relative change = 2.015e-03) \n....................................................................................................\nCompleted E-Step (7 seconds). \nCompleted M-Step. \nCompleting Iteration 5 (approx. per word bound = -7.197, relative change = 1.137e-03) \nTopic 1: obama, mccain, campaign, barack, john \n Topic 2: bill, legisl, vote, congress, senat \n Topic 3: bush, presid, white, hous, said \n Topic 4: democrat, senat, republican, will, hous \n Topic 5: get, one, ’re, ’ll, like \n Topic 6: elect, will, govern, parti, polit \n Topic 7: mccain, john, mccain’, tax, said \n Topic 8: vote, voter, elect, state, ballot \n Topic 9: russia, georgia, russian, world, nation \n Topic 10: one, will, film, like, say \n Topic 11: school, citi, polic, said, student \n Topic 12: wright, church, mccain, conserv, black \n Topic 13: obama, poll, mccain, voter, state \n Topic 14: iran, bush, presid, nuclear, polici \n Topic 15: iraq, iraqi, troop, war, militari \n Topic 16: court, right, law, tortur, constitut \n Topic 17: hillari, clinton, obama, democrat, will \n Topic 18: palin, news, media, report, time \n Topic 19: american, border, chavez, govern, trade \n Topic 20: oil, energi, global, will, price \n Topic 21: peopl, like, dont, think, know \n Topic 22: israel, attack, terrorist, will, kill \n Topic 23: report, investig, depart, said, offici \n Topic 24: will, tax, govern, economi, econom \n Topic 25: obama, barack, chicago, ayer, polit \n....................................................................................................\nCompleted E-Step (7 seconds). \nCompleted M-Step. \nCompleting Iteration 6 (approx. per word bound = -7.192, relative change = 6.888e-04) \n....................................................................................................\nCompleted E-Step (7 seconds). \nCompleted M-Step. \nCompleting Iteration 7 (approx. per word bound = -7.189, relative change = 4.437e-04) \n....................................................................................................\nCompleted E-Step (7 seconds). \nCompleted M-Step. \nCompleting Iteration 8 (approx. per word bound = -7.187, relative change = 3.036e-04) \n....................................................................................................\nCompleted E-Step (7 seconds). \nCompleted M-Step. \nCompleting Iteration 9 (approx. per word bound = -7.185, relative change = 2.178e-04) \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 10 (approx. per word bound = -7.184, relative change = 1.563e-04) \nTopic 1: obama, mccain, campaign, barack, john \n Topic 2: bill, vote, legisl, congress, senat \n Topic 3: bush, presid, white, hous, said \n Topic 4: democrat, republican, senat, gop, will \n Topic 5: get, one, like, ’re, doesn’t \n Topic 6: elect, will, govern, parti, polit \n Topic 7: mccain, john, mccain’, sen, said \n Topic 8: vote, voter, elect, state, ballot \n Topic 9: world, russia, nation, georgia, will \n Topic 10: one, will, film, like, allah \n Topic 11: school, citi, said, polic, student \n Topic 12: black, wright, church, conserv, white \n Topic 13: poll, obama, mccain, voter, state \n Topic 14: iran, nuclear, polici, foreign, iranian \n Topic 15: iraq, war, iraqi, troop, militari \n Topic 16: court, law, right, tortur, constitut \n Topic 17: hillari, clinton, obama, will, democrat \n Topic 18: palin, media, news, report, time \n Topic 19: immigr, illeg, border, american, trade \n Topic 20: oil, energi, price, global, will \n Topic 21: peopl, think, dont, like, know \n Topic 22: terrorist, attack, israel, terror, kill \n Topic 23: report, investig, depart, offici, said \n Topic 24: will, tax, economi, econom, money \n Topic 25: obama, barack, chicago, polit, ayer \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 11 (approx. per word bound = -7.183, relative change = 1.131e-04) \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 12 (approx. per word bound = -7.183, relative change = 8.554e-05) \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 13 (approx. per word bound = -7.182, relative change = 6.697e-05) \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 14 (approx. per word bound = -7.182, relative change = 5.235e-05) \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 15 (approx. per word bound = -7.181, relative change = 4.247e-05) \nTopic 1: obama, mccain, campaign, barack, john \n Topic 2: bill, vote, legisl, congress, senat \n Topic 3: bush, presid, said, hous, white \n Topic 4: democrat, republican, senat, gop, hous \n Topic 5: get, one, like, ’re, want \n Topic 6: will, elect, polit, parti, govern \n Topic 7: mccain, john, sen, mccain’, said \n Topic 8: vote, voter, elect, state, ballot \n Topic 9: world, nation, will, russia, georgia \n Topic 10: one, will, film, book, life \n Topic 11: school, citi, said, polic, famili \n Topic 12: black, wright, church, white, american \n Topic 13: poll, obama, mccain, state, voter \n Topic 14: iran, nuclear, foreign, polici, iranian \n Topic 15: iraq, war, iraqi, troop, militari \n Topic 16: court, law, right, tortur, constitut \n Topic 17: clinton, hillari, obama, will, democrat \n Topic 18: palin, media, news, report, time \n Topic 19: immigr, illeg, border, trade, american \n Topic 20: oil, energi, price, global, drill \n Topic 21: peopl, think, like, dont, know \n Topic 22: terrorist, attack, israel, terror, kill \n Topic 23: report, investig, depart, offici, former \n Topic 24: will, tax, economi, econom, money \n Topic 25: obama, barack, chicago, polit, ayer \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 16 (approx. per word bound = -7.181, relative change = 3.463e-05) \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 17 (approx. per word bound = -7.181, relative change = 2.854e-05) \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 18 (approx. per word bound = -7.181, relative change = 2.408e-05) \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 19 (approx. per word bound = -7.181, relative change = 2.000e-05) \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 20 (approx. per word bound = -7.181, relative change = 1.666e-05) \nTopic 1: obama, mccain, campaign, barack, john \n Topic 2: bill, vote, legisl, congress, senat \n Topic 3: bush, presid, said, administr, white \n Topic 4: democrat, republican, senat, gop, hous \n Topic 5: get, one, like, ’re, want \n Topic 6: will, elect, polit, parti, conserv \n Topic 7: mccain, john, sen, mccain’, said \n Topic 8: vote, voter, elect, state, ballot \n Topic 9: world, nation, will, america, countri \n Topic 10: one, will, film, book, life \n Topic 11: school, citi, said, famili, polic \n Topic 12: black, wright, church, white, american \n Topic 13: poll, mccain, obama, state, voter \n Topic 14: iran, nuclear, foreign, polici, iranian \n Topic 15: iraq, war, iraqi, troop, militari \n Topic 16: court, law, right, tortur, constitut \n Topic 17: clinton, hillari, obama, will, democrat \n Topic 18: palin, media, news, report, time \n Topic 19: immigr, illeg, border, trade, alien \n Topic 20: oil, energi, price, global, drill \n Topic 21: peopl, think, like, know, dont \n Topic 22: terrorist, attack, israel, kill, terror \n Topic 23: report, investig, depart, offici, former \n Topic 24: tax, will, economi, econom, plan \n Topic 25: obama, barack, chicago, campaign, polit \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 21 (approx. per word bound = -7.180, relative change = 1.415e-05) \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 22 (approx. per word bound = -7.180, relative change = 1.229e-05) \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nCompleting Iteration 23 (approx. per word bound = -7.180, relative change = 1.070e-05) \n....................................................................................................\nCompleted E-Step (6 seconds). \nCompleted M-Step. \nModel Converged \n\n\n\n\nCode\nplot(res_K)\n\n\n\n\n\nFrom the above, my feeling is that \\(K\\) being somewhere between 15 and 25 is where we want to be thinking. However, coherence drops quite rapidly as \\(K\\) is increased, so I might opt for \\(K = 15\\) in this case. You may well disagree. It’s a subjective choice in the end, so it’s find if you don’t agree and think it should be 25 or 10 or whatever you prefer."
  },
  {
    "objectID": "stm-explainer.html#fit-stm-with-no-covariates",
    "href": "stm-explainer.html#fit-stm-with-no-covariates",
    "title": "Structural Topic Models",
    "section": "Fit STM with no Covariates",
    "text": "Fit STM with no Covariates\nWe can now fit our first STM. Here we won’t use any covariates on topic prevalence. Just a very simple model to start off with.\n\n\nCode\nmodel1 &lt;- stm(documents = docs,\n              vocab = vocab,\n              K = 15,\n              data = meta,\n              init.type = \"Spectral\", \n              verbose = FALSE)\n\n\n\nExplore\nWe can start by doing some checks of the model, beginning with topic coherence. This gives us a measure of how often the top words of each topic co-occur in the documents (e.g. how often does “lynx” and “rewilding” appear together in the same topic?). In stm, semantic coherence is measured on a negative scale, so closer to 0 means higher coherence. That is, a topic with a score of -40 is more coherent than one with -80.\nEach value corresponds to one of the topics.\n\n\nCode\nsemanticCoherence(model1, docs)\n\n\n [1] -54.01384 -46.18649 -52.19125 -73.64842 -45.21362 -57.09193 -79.88974\n [8] -64.34934 -64.93084 -89.65010 -80.10995 -54.54471 -39.24183 -72.45012\n[15] -63.94915\n\n\nThe output is organised from topic 1 to topic 15. So here, it seems like topic 13 has the best coherence, while topic 10 has the worst.\nWe can pair this with exclusivity. As a reminder, this is a measure of how distinct a topic’s top words are compared to other topics.\n\n\nCode\nexclusivity(model1)\n\n\n [1] 9.828369 9.693257 9.583936 9.675580 9.268534 9.707071 9.663757 9.709532\n [9] 8.841380 9.434878 9.455630 9.673962 9.451958 9.566483 9.691479\n\n\nHere, we see that topic 1 has the highest exclusivity.\nThese outputs are useful but there’s a better way to visualise it. We can always extract these values and plot them out. The “ideal” topics will be in the top right, topics with both high coherence and exclusivity.\n\n\nCode\ncoh &lt;- semanticCoherence(model1, docs)\nexc &lt;- exclusivity(model1)\n\ndf &lt;- data.frame(Topic = 1:length(coh), Coherence = coh, Exclusivity = exc)\n\nggplot(df, aes(x = Coherence, y = Exclusivity, label = Topic)) +\n  geom_point() +\n  geom_text(nudge_y = 0.05, size = 3, check_overlap = TRUE) +\n  labs(title = \"Semantic Coherence vs. Exclusivity\",\n       x = \"Semantic Coherence\", y = \"Exclusivity\") +\n  theme_minimal()\n\n\n\n\n\nSTM provides different ways to list the top words in each topic. Each highlights different characteristics of a topic:\n\n\n\n\n\n\n\n\nLabel Type\nWhat It Does\nGood For…\n\n\n\n\nHighest Probability\nRanks words by how often they appear in the topic\nGeneral sense of what dominates the topic\n\n\nFREX\nBalances frequency and exclusivity (shared control parameter)\nClear topic interpretation with distinct words\n\n\nLift\nHighlights words that are rare overall but frequent in topic\nSpotting unique, niche vocabulary\n\n\nScore\nBayesian log odds of word being in this topic vs. others\nEmphasizing topic-distinguishing words\n\n\n\n\n\nCode\nlabelTopics(model1, n = 5)\n\n\nTopic 1 Top Words:\n     Highest Prob: mccain, obama, campaign, john, palin \n     FREX: palin, mccain, biden, mccain’, sarah \n     Lift: oct, palin, biden, “mccain, mccain” \n     Score: mccain, obama, palin, oct, campaign \nTopic 2 Top Words:\n     Highest Prob: bill, vote, democrat, senat, congress \n     FREX: legisl, pelosi, bill, amend, congress \n     Lift: legisl, telecom, co-sponsor, pelosi, fisa \n     Score: legisl, vote, congress, bill, republican \nTopic 3 Top Words:\n     Highest Prob: obama, senat, will, polit, barack \n     FREX: lieberman, blagojevich, governor, illinoi, rezko \n     Lift: blagojevich, chairmanship, rahm, blago, jindal \n     Score: chairmanship, obama, blagojevich, lieberman, rezko \nTopic 4 Top Words:\n     Highest Prob: bush, said, presid, administr, hous \n     FREX: tortur, cheney, justic, depart, interrog \n     Lift: mukasey, waterboard, gonzal, interrog, mcclellan \n     Score: mcclellan, tortur, interrog, detaine, attorney \nTopic 5 Top Words:\n     Highest Prob: get, one, like, ’re, don’t \n     FREX: ’ll, doesn’t, ’re, didn’t, don’t \n     Lift: widget, see-dubya, ingraham, beck, vis-avi \n     Score: ’re, widget, ’ll, obama’, don’t \nTopic 6 Top Words:\n     Highest Prob: elect, vote, voter, state, campaign \n     FREX: franken, ballot, coleman, acorn, registr \n     Lift: absente, chambliss, nrsc, registr, canvass \n     Score: canvass, franken, ballot, coleman, vote \nTopic 7 Top Words:\n     Highest Prob: oil, energi, will, price, global \n     FREX: energi, oil, drill, global, warm \n     Lift: mugab, carbon, gallon, gasolin, emiss \n     Score: mugab, oil, energi, drill, price \nTopic 8 Top Words:\n     Highest Prob: tax, will, econom, economi, govern \n     FREX: tax, health, financi, mortgag, economi \n     Lift: gramm, lender, mortgag, aig, fanni \n     Score: gramm, tax, mortgag, billion, bailout \nTopic 9 Top Words:\n     Highest Prob: one, will, women, life, peopl \n     FREX: film, life, allah, movi, god \n     Lift: vers, muhammad, allah, film, allah’ \n     Score: muhammad, allah, film, vers, women \nTopic 10 Top Words:\n     Highest Prob: attack, govern, will, terrorist, kill \n     FREX: russian, pakistan, russia, pakistani, taliban \n     Lift: pakistani, georgian, putin, russian, ukrain \n     Score: ossetia, russian, pakistan, russia, taliban \nTopic 11 Top Words:\n     Highest Prob: report, time, new, group, york \n     FREX: student, school, univers, newspap, publish \n     Lift: berkeley, campus, dohrn, copyright, annenberg \n     Score: berkeley, ayer, school, polic, student \nTopic 12 Top Words:\n     Highest Prob: obama, hillari, clinton, democrat, poll \n     FREX: hillari, clinton, romney, primari, deleg \n     Lift: zogbi, super-deleg, superdeleg, uncommit, hillari \n     Score: hillari, zogbi, obama, poll, clinton \nTopic 13 Top Words:\n     Highest Prob: think, peopl, like, dont, polit \n     FREX: dont, linktocommentspostcount, postcounttb, that, wright \n     Lift: hage, gasbag, digbyi, wingnut, youd \n     Score: hage, wright, dont, hes, linktocommentspostcount \nTopic 14 Top Words:\n     Highest Prob: iran, israel, nuclear, state, polici \n     FREX: israel, iran, hama, isra, iranian \n     Lift: palestinian, ahmadinejad, hama, israel, bolton \n     Score: bolton, iran, israel, iranian, hama \nTopic 15 Top Words:\n     Highest Prob: iraq, war, iraqi, militari, troop \n     FREX: iraqi, iraq, troop, withdraw, petraeus \n     Lift: basra, maliki, maliki’, nouri, al-maliki \n     Score: sadr, iraq, iraqi, troop, maliki \n\n\n\n\nCode\nplot(model1, type = \"summary\")\n\n\n\n\n\nWe can also extract sections that exemplify specific topics. This is how we learn what each topic is probably describing. We know the output is for topic \\(k\\), so what does that convey? For example, here’s an section specific to topic 1.\n\n\nCode\nfindThoughts(model1, texts = meta$documents, n = 1, topics = 1)\n\n\n\n Topic 1: \n     Oh, this is fun. Today the McCain campaign held a conference call unveiling a new \"truth squad\" Web site designed to defend McCain from attacks on his military record.   This was in response to Wes Clark's  claim yesterday that McCain lacks the necessary experience to be President, which wasn't an attack on McCain's military record at all.  Be that as it may,  on the call, the McCain camp rolled out a leading surrogate named Bud Day -- who was described merely as a fellow POW of McCain -- who blasted such attacks. \"John was slandered and reviled in the 2000 campaign in a way that denigrated his service enormously...it was absolutely important to face this issue right off the bat.\"  But guess what -- it turns out that this very same Bud Day was  featured in the Swift Boat Vets ads attacking John Kerry in 2004!  To make matters even better, recall that McCain himself  condemned the Swift Boat Vets. Yet now the McCain campaign is cheerfully enlisting someone who did what McCain claimed to decry -- attacks on Kerry's credentials -- and using him to defend McCain against the same sort of attacks.  That's a good one.  Late Update: As  Ben Smith notes, on the call Day defended his Swift Boat Vets work as being \"about laying out the truth.\"  Late Update: Here's the audio from the conference call:                                                                                                      Click To Play\n\n\nThis returns the document(s) that best represent a given topic. In this example, we ask for the most representative document (n = 1) for topic 1. Reading this helps us assign a human label to the topic.\nWith all of these summaries, I can start to get the sense that topic 1 has a theme related to Obama, McCain and their presidential campaigns. Therefore I might label this as Obama & McCain Campaigns.\n\n\nCleaner figure\nThe above figure is fine, but it’s a bit bland. Here’s a slightly jazzed up version:\n\n\nCode\ntopic_props &lt;- colMeans(model1$theta)\ntopic_sd &lt;- apply(model1$theta, 2, sd)\n\ntopic_df &lt;- data.frame(\n  Topic = factor(1:length(topic_props)),\n  Mean = topic_props,\n  SD = topic_sd\n)\n\ntop_frex &lt;- labelTopics(model1, n = 3)$frex\ntopic_labels &lt;- apply(top_frex, 1, function(words) paste(words, collapse = \", \"))\n\ntopic_df$Label &lt;- paste0(\"T\", topic_df$Topic, \": \", topic_labels)\n\nggplot(topic_df, aes(x = Label, y = Mean)) +\n  geom_col(fill = \"#00A68A\") +\n  geom_errorbar(aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.2, color = \"white\") +\n  labs(x = \"Topic\",\n       y = \"Expected %\\nof Corpus\") +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal(base_size = 12) +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1))\n\n\n\n\n\nWe can also use the results to create word clouds, showing the words that best capture the topic.\n\n\nCode\nlibrary(wordcloud)\ncloud(model1, topic = 1)\n\n\n\n\n\nOr the jazzier version:\n\n\nCode\ntopic_id &lt;- 1\ntop_words &lt;- labelTopics(model1, n = 50)$frex[topic_id, ]\n\nbeta_matrix &lt;- exp(model1$beta$logbeta[[1]])\nword_probs &lt;- beta_matrix[topic_id, ]\n\nvocab &lt;- model1$vocab\ndf &lt;- data.frame(\n  word = vocab,\n  prob = word_probs\n) |&gt; \n  filter(word %in% top_words)\n\nlibrary(ggwordcloud)\nggplot(df, aes(label = word, size = prob)) +\n  geom_text_wordcloud(area_corr = TRUE, color = \"#00A68A\") +\n  scale_size_area(max_size = 50) +\n  theme_minimal()\n\n\n\n\n\nThese tools allow us to evaluate the structure and interpretability of our STM model. From here, we can either adjust the number of topics, clean the vocabulary further, or move on to analyzing topic prevalence with covariates."
  },
  {
    "objectID": "stm-explainer.html#fit-stm-with-covariates",
    "href": "stm-explainer.html#fit-stm-with-covariates",
    "title": "Structural Topic Models",
    "section": "Fit STM with Covariates",
    "text": "Fit STM with Covariates\nNow that we’ve explored the basic STM model, we can move on to a more powerful feature: including covariates. This lets us ask questions like:\n\nDoes political leaning affect which topics are used?\nDo certain topics become more or less common over time?\n\nWe specify covariates in the same way as you did in BI3010, using formulas like ~ rating + day. But here we’re going to do something slightly different with time: we assume that the effect of day isn’t a straight line. Instead, it might wiggle, maybe topic prevalence rises, dips, and rises again over time (yes, the technical term for this really is wiggly).\nTo model this, we use a smooth term: s(day, df = 5).\n\ns() means “fit a smooth curve”\ndf stands for degrees of freedom, which controls how wiggly the curve can be\nA larger df allows more bends; a smaller one keeps the curve gentle\n\nThere’s no exact science to choosing df. In this case, I picked 5 because I have 5 fingers, and because the 2008 presidential campaign probably had some changes in topic focus, but not pure chaos every day.\nWe now get to the point where we can include covariates into the model. We do this in the same way that we did for the models in BI3010, using syntax like y ~ x + z. Here we’ll run a model that assumes that rating affects prevalence (\\(\\theta\\)), as well as day.\n\n\nCode\nmodel2 &lt;- stm(documents = docs,\n              vocab = vocab,\n              K = 15,\n              prevalence = ~ rating + s(day, df = 5),\n              data = meta,\n              init.type = \"Spectral\",\n              verbose = FALSE)\n\n\n\nExplore\nWe can begin by checking the coherence of each topic, just as we did before.\n\n\nCode\nsemanticCoherence(model2, docs)\n\n\n [1] -58.27322 -48.64337 -50.42340 -75.48279 -44.66076 -59.90875 -78.18520\n [8] -65.36588 -64.93084 -89.65010 -72.62134 -54.95509 -39.24183 -72.45012\n[15] -63.94915\n\n\nThese values are again negative, so closer to 0 means higher coherence. If topic 13 has a value of -40, and topic 10 has -85, then topic 13 is more interpretable.\nWe can pair this with exclusivity to see how distinct each topic is.\n\n\nCode\nexclusivity(model2)\n\n\n [1] 9.774044 9.716968 9.544814 9.705059 9.271672 9.712446 9.656976 9.647156\n [9] 8.853296 9.437992 9.354623 9.666774 9.475512 9.552169 9.689942\n\n\nLet’s visualise these two together to spot “ideal” topics, the ones that are both coherent and exclusive.\n\n\nCode\ncoh &lt;- semanticCoherence(model2, docs)\nexc &lt;- exclusivity(model2)\n\ndf &lt;- data.frame(Topic = 1:length(coh), Coherence = coh, Exclusivity = exc)\n\nggplot(df, aes(x = Coherence, y = Exclusivity, label = Topic)) +\n  geom_point() +\n  geom_text(nudge_y = 0.05, size = 3, check_overlap = TRUE) +\n  labs(x = \"Semantic Coherence (closer to 0 is better)\", \n       y = \"Exclusivity (higher is better)\") +\n  theme_minimal()\n\n\n\n\n\nAnd we can check the top words per topic for our new model:\n\n\nCode\nlabelTopics(model2, n = 5)\n\n\nTopic 1 Top Words:\n     Highest Prob: mccain, campaign, obama, john, palin \n     FREX: palin, mccain, biden, mccain’, sarah \n     Lift: oct, palin, “mccain, mccain”, sarah \n     Score: mccain, palin, oct, obama, mccain’ \nTopic 2 Top Words:\n     Highest Prob: democrat, bill, vote, senat, republican \n     FREX: legisl, pelosi, bill, reid, amend \n     Lift: legisl, telecom, pelosi, co-sponsor, demint \n     Score: legisl, vote, congress, republican, pelosi \nTopic 3 Top Words:\n     Highest Prob: obama, barack, campaign, senat, will \n     FREX: illinoi, barack, blagojevich, chicago, lieberman \n     Lift: chairmanship, blagojevich, rahm, blago, rezko \n     Score: obama, chairmanship, blagojevich, barack, lieberman \nTopic 4 Top Words:\n     Highest Prob: bush, presid, said, administr, hous \n     FREX: tortur, cheney, justic, attorney, interrog \n     Lift: mukasey, waterboard, gonzal, interrog, mcclellan \n     Score: mcclellan, tortur, detaine, interrog, attorney \nTopic 5 Top Words:\n     Highest Prob: get, one, like, ’re, don’t \n     FREX: doesn’t, ’ll, didn’t, ’re, don’t \n     Lift: widget, see-dubya, ingraham, vis-avi, media’ \n     Score: ’re, widget, ’ll, obama’, don’t \nTopic 6 Top Words:\n     Highest Prob: elect, vote, voter, state, republican \n     FREX: franken, ballot, coleman, acorn, registr \n     Lift: absente, chambliss, nrsc, registr, canvass \n     Score: canvass, franken, ballot, coleman, vote \nTopic 7 Top Words:\n     Highest Prob: oil, energi, will, price, global \n     FREX: energi, oil, drill, climat, warm \n     Lift: carbon, mugab, gallon, gasolin, anwr \n     Score: mugab, oil, energi, drill, price \nTopic 8 Top Words:\n     Highest Prob: tax, will, econom, economi, govern \n     FREX: tax, health, financi, mortgag, economi \n     Lift: fanni, gramm, lender, mortgag, aig \n     Score: gramm, tax, mortgag, billion, bailout \nTopic 9 Top Words:\n     Highest Prob: one, will, women, life, peopl \n     FREX: film, life, allah, god, women \n     Lift: vers, allah, muhammad, film, allah’ \n     Score: muhammad, allah, film, vers, women \nTopic 10 Top Words:\n     Highest Prob: govern, attack, will, terrorist, kill \n     FREX: russian, pakistan, russia, pakistani, taliban \n     Lift: pakistani, bhutto, moscow, musharraf, putin \n     Score: ossetia, russian, pakistan, russia, taliban \nTopic 11 Top Words:\n     Highest Prob: report, time, new, stori, group \n     FREX: student, ayer, school, newspap, univers \n     Lift: berkeley, dohrn, campus, annenberg, copyright \n     Score: berkeley, ayer, school, polic, student \nTopic 12 Top Words:\n     Highest Prob: obama, hillari, clinton, democrat, poll \n     FREX: hillari, romney, clinton, primari, deleg \n     Lift: zogbi, super-deleg, superdeleg, uncommit, romney \n     Score: hillari, zogbi, obama, poll, clinton \nTopic 13 Top Words:\n     Highest Prob: think, peopl, like, dont, say \n     FREX: dont, linktocommentspostcount, postcounttb, that, didnt \n     Lift: hage, gasbag, digbyi, youd, tristero \n     Score: hage, wright, dont, linktocommentspostcount, postcounttb \nTopic 14 Top Words:\n     Highest Prob: iran, israel, nuclear, state, polici \n     FREX: israel, iran, isra, hama, iranian \n     Lift: palestinian, ahmadinejad, gaza, hama, isra \n     Score: bolton, iran, israel, iranian, hama \nTopic 15 Top Words:\n     Highest Prob: iraq, war, iraqi, militari, troop \n     FREX: iraqi, iraq, troop, withdraw, petraeus \n     Lift: basra, maliki, maliki’, al-maliki, al-sadr \n     Score: sadr, iraq, iraqi, troop, maliki \n\n\nAnd, as before, we can extract text that exemplifies each topic — a helpful way to figure out what a topic might mean.\n\n\nCode\nfindThoughts(model2, texts = meta$documents, n = 1, topics = 1)\n\n\n\n Topic 1: \n     Infuriated About Tough CNN Interview, McCain Cancels Larry King Appearance                     Yesterday, Sen. John McCain’s (R-AZ) campaign spokesman Tucker Bounds appeared on CNN for an interview with Campbell Brown. Brown was tough on Bounds, refusing to let him spout typical campaign talking points. She repeatedly pressed him on Palin’s foreign policy experience and qualifications, asking him to name one decision that she made as commander-in-chief of the Alaskan National Guard. Bounds was unable to do so. Today, CNN’s Wolf Blitzer revealed that because of that tough interview, the McCain campaign has canceled the senator’s appearance on Larry King Live tonight: The McCain campaign said it believed that exchange was over the line and as a result the interview scheduled for Larry King Live with Sen. McCain was pulled. CNN does not believe that Campbell’s interview was over the line. We are committed to fair coverage of both sides of this historic election. CNN also replayed the interview between Brown and Bounds. Watch Blitzer’s announcement and the interview: The McCain campaign has repeatedly tried to intimidate the press. It is now angry about media coverage of Bristol Palin’s pregnancy, calling NBC’s reporting on it “irresponsible journalism.” Campaign staffers “even considered pulling out of one of the three presidential debates because it would be moderated by Tom Brokaw, a former NBC News anchorman.” When Newsweek wrote a cover story in May examining the hardball tactics conservatives might use in the general election, the McCain campaign “threatened to throw the magazine’s reporters off the campaign bus and airplane.” Digg It!\n\n\nThis returns the most representative document for the selected topic. Reading it helps guide our interpretation. This time around in our new model, for Topic 1, it seems to be more closely focussed on McCain’s Campaign (and possibly attacks on it) rather than also including Obama per se.\n\n\nCleaner figure\nWe can recreate the topic prevalence plot using ggplot2, just like before.\n\n\nCode\ntopic_props &lt;- colMeans(model2$theta)\ntopic_sd &lt;- apply(model2$theta, 2, sd)\n\ntopic_df &lt;- data.frame(\n  Topic = factor(1:length(topic_props)),\n  Mean = topic_props,\n  SD = topic_sd\n)\n\ntop_frex &lt;- labelTopics(model2, n = 3)$frex\ntopic_labels &lt;- apply(top_frex, 1, function(words) paste(words, collapse = \", \"))\n\ntopic_df$Label &lt;- paste0(\"T\", topic_df$Topic, \": \", topic_labels)\n\nggplot(topic_df, aes(x = Label, y = Mean)) +\n  geom_col(fill = \"#00A68A\") +\n  geom_errorbar(aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.2, color = \"white\") +\n  labs(x = \"Topic\", y = \"Expected %\\nof Corpus\") +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal(base_size = 12) +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1))\n\n\n\n\n\n\n\nWord Clouds\nLet’s also generate word clouds for a topic in the model with covariates.\n\n\nCode\ntopic_id &lt;- 1\ntop_words &lt;- labelTopics(model2, n = 50)$frex[topic_id, ]\n\nbeta_matrix &lt;- exp(model2$beta$logbeta[[1]])\nword_probs &lt;- beta_matrix[topic_id, ]\n\nvocab &lt;- model2$vocab\ndf &lt;- data.frame(\n  word = vocab,\n  prob = word_probs\n) |&gt; \n  filter(word %in% top_words)\n\nggplot(df, aes(label = word, size = prob)) +\n  geom_text_wordcloud(area_corr = TRUE, color = \"#00A68A\") +\n  scale_size_area(max_size = 50) +\n  theme_minimal()"
  },
  {
    "objectID": "stm-explainer.html#estimate-and-plot-covariate-effects",
    "href": "stm-explainer.html#estimate-and-plot-covariate-effects",
    "title": "Structural Topic Models",
    "section": "Estimate and Plot Covariate Effects",
    "text": "Estimate and Plot Covariate Effects\nNow that we’ve included covariates, we can test and plot how topic prevalence varies by metadata. This is where STM becomes really powerful.\n\n\nCode\neffect_model &lt;- estimateEffect(1:15 ~ rating + s(day), model2,\n                               meta = meta, uncertainty = \"Global\")\n\n\nWhich we can use to plot how a topic changes over time. For example, how did Topic 7’s prevalence change over time?\n\n\nCode\nplot(effect_model, \"day\", method = \"continuous\", topics = 7,\n     printlegend = FALSE, xlab = \"Time (2008)\", xaxt = \"n\")\n\nmonthseq &lt;- seq(from = as.Date(\"2008-01-01\"), to = as.Date(\"2008-12-01\"), by = \"month\")\nmonthnames &lt;- months(monthseq)\naxis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)), labels = monthnames)\n\n\n\n\n\n\n\nCode\nplot(effect_model, covariate = \"rating\", topics = 3, method = \"difference\",\n     cov.value1 = \"Liberal\", cov.value2 = \"Conservative\",\n     xlab = \"&lt;- More Liberal | More Conservative -&gt;\",\n     main = \"Topic 3: Difference by Political Rating\")\n\n\n\n\n\nThese figures are… ok but I think we can do better. Here’s my attempt:\n\n\nCode\nggplot(plot_data, aes(x = day, y = est, color = topic)) +\n  geom_line(linewidth = 1) +\n  geom_ribbon(aes(ymin = ci.lower, ymax = ci.upper, fill = topic), alpha = 0.15, colour = NA) +\n  labs(x = \"Time\",\n       y = \"Estimated Topic Proportion\",\n       color = \"Topic\",\n       fill = \"Topic\") +\n  theme_minimal(base_size = 13)"
  }
]