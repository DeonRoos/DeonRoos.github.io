---
title: "Structural Topic Models"
author: "Deon Roos"
format: 
  html:
    theme: flatly
    highlight: monochrome
    code-fold: true
    toc: true
    toc-depth: 2
    toc-location: left
    css: styles.css
---

```{r setup}
#| include: false
#| message: false
#| warning: false
library(knitr)
opts_chunk$set(echo = TRUE)
```

Hey Grace, this page has been written especially for you. I've tried to write the material here to help cover the underlying theory of STMs as well as how to actually *implement* them. But be warned, the theory here is tough and I'm new to this myself. It took me a good while to make sense of the stats here, so be patient with yourself (and me). This page is not intended to be your "supervisor" - I'm still here to help you. Instead, it's intended to be a stand alone document you can always come back to whenever you need.

Hopefully this page helps you understand the method and is useful, but let me know if you have any doubts. It's unlikely that this walk through will answer every bit of confusion you have so I won't take any offence if you want to burn this in a giant pyre. Just tell me.

------------------------------------------------------------------------

# Why analyse text?

Let's say we want to understand how movie critic reviews have changed over time. How would we do this. Well, you say, maybe the "simplest" way is simply to read thousands of critic reviews over 50 years and try to summarise this ourselves. That's damned hard but do-able. That's what a lot of arts PhD students do. I'd argue I'm not smart enough to do that, so I want to cheat and use computers to do it for me.

But how do we do that?

This is where you are moving into a murky area; somewhere between statistics and machine learning (I'm not entirely sure where STM falls in this regard).

One option is to use *supervised* learning. This is where you specify a goal and try to achieve it. For example, in BI3010, you learnt how to do *supervised* learning using linear models:

$$
y_i \sim Normal(\mu_i, \sigma) \\
\mu_i = \beta_0 + \beta_1 \times x_i
$$

Here our goal was to learn $\beta_0$ and $\beta_1$. This is described as *supervised* because we have told the computer what it should learn; "learn these straight line parameters".

To do this with text would require that you sit down and label each document that comes across your desk at being "Pro-Lynx" or "Anti-Lynx". Again, do-able, but that doesn't seem like much of an upgrade compared to the first arts PhD option.

The last option is *unsupervised* learning. In contrast with *supervised*, we don't have a goal *per se* here. Instead, our goal is to *discover* the goal. What that means is that we are going to use an algorithm to discover themes and patterns in the text that we did not know to look for. Our job is to interpret these results; What does the first theme actually mean? That's much easier than trying to identify the theme ourselves!

There are no shortage of options available out there to do this, but the one you'll use in your thesis is called Structural Topic Modelling.

## What is a Structural Topic Model?

A *Structural Topic Model (STM)* is a type of analysis that allows us to explore large sets of text documents and identify what the common themes are; called *topics*. Just like you did in BI3010, you can also include covariates (also called explanatory variables) to say why you think a topic may be more or less prevalent over time.

> **Grace**, in your case, this might include things like how date (i.e. how has coverage of lynx changed over time), and if the illegal release of lynx (and maybe their interaction) may have changed which topics are more or less prevelant. For example, *did negative topics become more common in the media after the illegal release compared to before*?

STMs are a fairly complex beast. There are lots of elements that we could cover. One that I won't touch on much here is *Bayesian* statistics. I have written another set of [documents](https://deonroos.github.io/Occupancy_Modelling/Bayesian_Statistics.html) that explain the general theory of this, which you are welcome to see. Although STMs, as implemented in the `R` package `stm`, use the *Bayesian* statistical framework, you can't actually interact with it, so it's not crucial to understand in this case. However, I would recommend trying to wrap your head around it, as it's a piece of knowledge that may make you highly employable.

Structural Topic Models make the following key assumptions:

-   Each *document* (e.g., a newspaper article) is a *mixture of topics*, with each topic contributing to the document in a proportion.\
    *Example*: A particular article might be 90% about "lynx are good" and 5% about "lynx are bad"

-   Each *word* in the document is associated with one of these topics.\
    *Example*: If a word belongs to the topic "lynx are bad", it is more likely to be something like "kill", "predator", or "livestock". The probabilty for topic-specific word contribute to create an entire distribution of probabilities for all words used in a topic. Collectively, topic-words are referred to as *content*.

Thus, each *topic* has a characteristic *distribution over words*, e.g., topic 3 might have a 30% chance of using "kill", 25% chance of "livestock", and only 0.5% for "natural".

So our objective then, is to identify different *topics*, and which words tend to categorise those topics. This is what we're, fundamentally, trying to do in an STM.

## But how do they work?

Mechanically, an STM works as follows:

1.  For each document $d$, estimate the *latent* topic prevalences (*latent* just means something we cannot observe, i.e. there's no label on a newspaper article that says "This is Pro-Lynx", we have to estimate that from data). We do this by using a $logistic$ $normal$ distribution, whose mean is determined by document-level covariates (basically through a GLM).

2.  Convert these *latent* prevalences into a topic proportion vector $\theta_d$ via a softmax transformation (i.e., normalizing onto the simplex).

3.  For each word $w_{d,n}$ in the document:

    -   Estimate which *topic* to assign it to using $z_{d,n} \sim \text{Categorical}(\theta_d)$ (another GLM)

    -   Then, given the topic $z_{d,n}$, estimate the word from that topic's word distribution $\beta{z_{d,n}}$

That's only 5 bullet points and it made my head spin multiple times the first time I read it. By the time I wrote it (and re-wrote it multiple times to try and make it make sense) my head was doing cartwheels. It sounds horrible. It reads horribly. But in truth, it's not *that* bad. I genuinely believe you can wrap your head around it without any cartwheels.

# What does it look like?

One way to visualise an STM is by using *plate notation*:

```{r}
library(DiagrammeR)

grViz("
digraph stm {
  graph [layout = dot, rankdir = LR]

  # Nodes
  Σ [shape=circle, label='Σ', style=dashed]
  γ [shape=circle, label='γ', style=dashed]
  X [shape=circle, label='X']
  Y [shape=circle, label='Y']
  θ [shape=circle, label='θ', style=dashed]
  z [shape=circle, label='z', style=dashed]
  w [shape=circle, label='w']
  β [shape=circle, label='β', style=dashed]

  # Edges
  Σ -> θ
  γ -> θ
  X -> θ
  θ -> z
  z -> w
  β -> w
  Y -> β

  # Outer plate: D
  subgraph cluster_D {
    label = 'D'
    style = 'solid'
    X; θ; β; Y;

    # Nested plate: N
    subgraph cluster_N {
      label = 'N'
      style = 'solid'
      z; w;
    }
  }
}
")

```

Where:

-   *Nodes*: Circles represent variables. Dashed circles mean they are *latent* (a variable we have to estimate), while solid circles are observed data.

-   *Plates*: Rectangle indicate repetition:

    -   $D$: For each document

    -   $N$: For each word in a document

And where the variables in the plate notation are:

-   $X$ - Document level covariates (e.g. date of publication, political leaning)

-   $\alpha$ - The regression coefficients for the mean of the logistic normal

-   $\mu$ - The mean *vector* for the multivariate normal (logit-scale topic propensities)

-   $\Sigma$ - The *covariance* *matrix* between topics (models topic co-occurence)

-   $z_d$ - Latent logit-scale topic propensity for document $d$

-   $\theta$ - Topic proportions for a document (which sums to 1)

-   $z_{d,n}$ - Topic assignment for word $n$ in document $d$

-   $w_{d,n}$ - The observed word

-   $\beta_k$ - Word distribution (categorical) for topic $k$.

Here's what that looks like in a less "stats" way and with a few details omitted (also apologies for the generative AI image):

![](media/lynx_stm.png)

## Mathematical formulation

The core structure of an STM is:

$$
\begin{aligned}
\eta_d &\sim \mathcal{MVNorm}(X_d \Gamma, \Sigma) \\
\theta_d &= \text{softmax}(\eta_d) \\
z_{d,n} &\sim \text{Categorical}(\theta_d) \\
w_{d,n} &\sim \text{Categorical}(\beta_{z_{d,n}}) \\
\beta_{k,v} \propto \exp\left( m_v + \kappa_{k,v}^{(\text{topic})} + \kappa_{g,v}^{(\text{cov})} + \kappa_{k,g,v}^{(\text{int})} \right) \\
\end{aligned}
$$

$\eta_d$ is a *latent* vector of logit-scale topic propensities for document $d$, drawn from a multivariate normal distribution with mean $X_d\Gamma$ and covariance $\Sigma$. (Rather than there being a single conditional mean like in a $Normal$ linear model, here $X_d\Gamma$ is a column, or *vector* of means, one for each document).

$\theta_d$ is the topic proportion *vector* for document $d$, obtained by applying a softmax transformation to $\eta_d$. All entries are non-negative and sum to 1.

$z_{d,n}$ is the *latent* topic assignment for word $n$ in document $d$, drawn from a categorical distribution with probabilities $\theta_d$.

$w_{d,n}$ is the actual observed word, drawn from the categorical distribution $\beta_z_{d,n}$, i.e., from the word distribution corresponding to the assigned topic.

$\beta$ is the topic--word matrix, of dimension $\mathbf{K}\times \mathbf{V}$, where each row $\beta_k$ is a probability distribution over the vocabulary (words) for topic $k$ (each row in the $\beta$ matrix below). $\beta$ can be estimated from the data or modeled as a logit-linear function of content covariates.

For $\beta$ it's actually a topic-word matrix $\beta$: $\mathbf{K} \times \mathbf{V}$

$$
\begin{array}{c|cccc}
& \text{predator} & \text{policy} & \cdots & \text{illegal} \\
\hline
\text{Topic } 1 & \beta_{1,1} & \beta_{1,2} & \cdots & \beta_{1,V} \\
\text{Topic } 2 & \beta_{2,1} & \beta_{2,2} & \cdots & \beta_{2,V} \\
\text{Topic } 3 & \beta_{3,1} & \beta_{3,2} & \cdots & \beta_{3,V} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\text{Topic } K & \beta_{K,1} & \beta_{K,2} & \cdots & \beta_{K,V}
\end{array}
$$

And for $\theta$, it's a document-topic matrix: $\mathbf{D} \times \mathbf{K}$

$$
\begin{array}{c|cccc}
& \text{Topic 1} & \text{Topic 2} & \cdots & \text{Topic } K \\
\hline
Doc1 & \theta_{1,1} & \theta_{1,2} & \cdots & \theta_{1,K} \\
Doc2 & \theta_{2,1} & \theta_{2,2} & \cdots & \theta_{2,K} \\
Doc3 & \theta_{3,1} & \theta_{3,2} & \cdots & \theta_{3,K} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
D & \theta_{D,1} & \theta_{D,2} & \cdots & \theta_{D,K}
\end{array}
$$

## Structural Topic Models Explained (Video)

<iframe width="560" height="315" src="https://www.youtube.com/embed/3kcjbzy4UPM?start=828" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>

</iframe>

------------------------------------------------------------------------

## Load Packages and Data

```{r}
#| label: load-packages
#| cache: true
library(stm)
library(tm)
library(ggplot2)

# Load the data
data <- read.csv("data/poliblogs2008.csv", stringsAsFactors = FALSE)

# Display just the first few rows
head(data, 5)
```

------------------------------------------------------------------------

## Preprocess Text

```{r}
#| label: preprocess-text
#| cache: true
processed <- textProcessor(data$documents, metadata = data)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)

docs <- out$documents
vocab <- out$vocab
meta <- out$meta
```

```{r}
#| label: plot-removed
#| cache: true
plotRemoved(processed$documents, lower.thresh = seq(1, 200, by = 100))
```

------------------------------------------------------------------------

## Choosing K (Number of Topics)

```{r}
#| label: choose-k
#| eval: false
#| cache: true
# searchK(docs, vocab, K = c(10, 15, 20, 25), data = meta)
```

Expectation-Maximiation equivalent to brute forcing MCMC - give it a function and iteratively optimise it until threshold of error reached. Priors locked (I think)

------------------------------------------------------------------------

## Fit STM -- Model 1 (No Covariates)

```{r}
#| label: model1
#| cache: true
model1 <- stm(documents = docs,
              vocab = vocab,
              K = 20,
              data = meta,
              init.type = "Spectral", 
              verbose = FALSE)
```

### Explore Model 1

```{r}
#| label: model1-explore
#| cache: true
plot(model1, type = "summary")
labelTopics(model1)
findThoughts(model1, texts = meta$documents, n = 2, topics = 3)
```

------------------------------------------------------------------------

## Fit STM -- Model 2 (With Covariates)

```{r}
#| label: model2
#| cache: true
model2 <- stm(documents = docs,
              vocab = vocab,
              K = 20,
              prevalence = ~ rating + s(day, df = 5),
              data = meta,
              init.type = "Spectral",
              verbose = FALSE)
```

### Explore Model 2

```{r}
#| label: model2-explore
#| cache: true
plot(model2, type = "summary")
labelTopics(model2, topics = c(3, 7, 20))
findThoughts(model2, texts = meta$documents, n = 2, topics = 3)$docs[[1]]
```

------------------------------------------------------------------------

## Estimate and Plot Covariate Effects

```{r}
#| label: estimate-effect
#| cache: true
effect_model <- estimateEffect(1:20 ~ rating + s(day), model2,
                               meta = meta, uncertainty = "Global")
summary(effect_model, topics = 3)
```

```{r}
#| label: plot-time
#| cache: true
plot(effect_model, "day", method = "continuous", topics = 7,
     printlegend = FALSE, xlab = "Time (2008)", xaxt = "n")

monthseq <- seq(from = as.Date("2008-01-01"), to = as.Date("2008-12-01"), by = "month")
monthnames <- months(monthseq)
axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)), labels = monthnames)
```

```{r}
#| label: plot-rating-diff
#| cache: true
plot(effect_model, covariate = "rating", topics = 3, method = "difference",
     cov.value1 = "Liberal", cov.value2 = "Conservative",
     xlab = "More Liberal ... More Conservative",
     main = "Topic 3: Difference by Political Rating")
```

------------------------------------------------------------------------

## Summary

-   STM helps uncover underlying topics in a collection of documents.
-   Document-level metadata can be used to model how topic proportions vary.
-   The logistic normal plays a similar role to a logit link in GLMs.
-   STM is hierarchical: topic use varies across documents; topics generate words.
